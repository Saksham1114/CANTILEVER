{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474158e7",
        "outputId": "15582a06-38dc-44cb-b7ff-5cccca8159e2"
      },
      "source": [
        "# Cell 3 - train small SentencePiece and tiny seq2seq (fast) - Retry\n",
        "from pathlib import Path\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "DATA = BASE / \"data\"\n",
        "MODELS = BASE / \"models\"\n",
        "CORPUS = DATA / \"corpus.txt\"\n",
        "\n",
        "# import helpers\n",
        "import sys\n",
        "# Ensure the path is only appended if not already present\n",
        "if str(BASE) not in sys.path:\n",
        "    sys.path.append(str(BASE))\n",
        "from utils import parse_srt_to_lines, build_pairs_from_lines\n",
        "\n",
        "# collect lines and pairs\n",
        "lines=[]\n",
        "for p in DATA.glob(\"*.srt\"):\n",
        "    lines += parse_srt_to_lines(p)\n",
        "print(\"Lines found:\", len(lines))\n",
        "pairs = build_pairs_from_lines(lines)\n",
        "print(\"Pairs:\", len(pairs))\n",
        "if len(pairs) == 0:\n",
        "    raise SystemExit(\"No pairs found. Upload .srt to /content/bolly_chatbot/data/ and rerun.\")\n",
        "\n",
        "# write corpus\n",
        "with open(CORPUS, \"w\", encoding=\"utf-8\") as f:\n",
        "    for a,b in pairs:\n",
        "        f.write(a + \"\\n\")\n",
        "        f.write(b + \"\\n\")\n",
        "print(\"Wrote corpus:\", CORPUS)\n",
        "\n",
        "# train SentencePiece (small vocab)\n",
        "import sentencepiece as spm\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "# Ensure SentencePiece model is trained only if it doesn't exist\n",
        "if not (Path(sp_prefix + \".model\")).exists():\n",
        "    spm.SentencePieceTrainer.Train(f\"--input={CORPUS} --model_prefix={sp_prefix} --vocab_size=128 --character_coverage=0.9995 --model_type=bpe --user_defined_symbols=<s>,</s>\")\n",
        "    print(\"Trained SPM:\", sp_prefix + \".model\")\n",
        "else:\n",
        "    print(\"SentencePiece model already exists:\", sp_prefix + \".model\")\n",
        "\n",
        "# prepare encoded data\n",
        "sp = spm.SentencePieceProcessor(); sp.Load(sp_prefix + \".model\")\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def encode(s, maxlen=16):\n",
        "    txt = \"<s> \" + s + \" </s>\"\n",
        "    # Ensure truncation happens if needed\n",
        "    encoded_ids = sp.EncodeAsIds(txt)\n",
        "    return encoded_ids[:maxlen]\n",
        "\n",
        "max_enc=16; max_dec=15\n",
        "encs=[]; decins=[]; decouts=[]\n",
        "for a,b in pairs:\n",
        "    e = encode(a,max_enc)\n",
        "    d = encode(b,max_dec+1)\n",
        "    if len(d) < 2: continue\n",
        "    encs.append(e)\n",
        "    decins.append(d[:-1])\n",
        "    decouts.append(d[1:])\n",
        "\n",
        "import numpy as np\n",
        "encs = pad_sequences(encs, maxlen=max_enc, padding='post')\n",
        "decins = pad_sequences(decins, maxlen=max_dec, padding='post')\n",
        "decouts = pad_sequences(decouts, maxlen=max_dec, padding='post')\n",
        "decouts = np.expand_dims(decouts, -1)\n",
        "\n",
        "# define tiny seq2seq\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "V = sp.GetPieceSize()\n",
        "emb_dim=48; units=48\n",
        "\n",
        "# Encoder\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "_, sh, sc = enc_lstm(emb_e)\n",
        "\n",
        "# Decoder\n",
        "dec_input = Input(shape=(max_dec,), name='dec_input')\n",
        "dec_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "# Add Reshape layer to explicitly set shape to (max_dec, emb_dim)\n",
        "reshaped_emb_d = Reshape((max_dec, emb_dim))(emb_d)\n",
        "dec_lstm = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs, _, _ = dec_lstm(reshaped_emb_d, initial_state=[sh, sc])\n",
        "dec_dense = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits = dec_dense(dec_outs)\n",
        "\n",
        "\n",
        "model = Model([enc_input, dec_input], logits)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "print(\"Training tiny model...\")\n",
        "model.fit([encs, decins], decouts, batch_size=4, epochs=4, verbose=2)\n",
        "\n",
        "# save artifacts\n",
        "model.save_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "# Correct method to save SentencePiece model is not needed as it's saved during training\n",
        "# The SentencePieceProcessor object does not have a 'Save' method.\n",
        "# The model is saved during the SentencePieceTrainer.Train call.\n",
        "print(\"Saved weights to\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "print(\"SPM vocab size:\", sp.GetPieceSize())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines found: 3\n",
            "Pairs: 2\n",
            "Wrote corpus: /content/bolly_chatbot/data/corpus.txt\n",
            "SentencePiece model already exists: /content/bolly_chatbot/models/spm_bolly.model\n",
            "Training tiny model...\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_3' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 4s - 4s/step - loss: 4.8527\n",
            "Epoch 2/4\n",
            "1/1 - 0s - 64ms/step - loss: 4.8460\n",
            "Epoch 3/4\n",
            "1/1 - 0s - 61ms/step - loss: 4.8392\n",
            "Epoch 4/4\n",
            "1/1 - 0s - 62ms/step - loss: 4.8323\n",
            "Saved weights to /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "SPM vocab size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2d83d1e",
        "outputId": "9fee0003-67d6-4ed7-ccdf-9ce8991c4799"
      },
      "source": [
        "# Cell 4 - Inference\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape # Import Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix + \".model\")\n",
        "V = sp.GetPieceSize()\n",
        "print(\"Loaded SPM model from:\", sp_prefix + \".model\")\n",
        "print(\"SPM vocab size:\", V)\n",
        "\n",
        "# Model parameters (should match training)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "# Recreate the full model structure used during training to load weights\n",
        "# Encoder\n",
        "enc_input_full = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e_full = enc_emb_layer_full(enc_input_full)\n",
        "enc_lstm_full = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "enc_outputs_full, sh_full, sc_full = enc_lstm_full(emb_e_full)\n",
        "\n",
        "# Decoder\n",
        "dec_input_full = Input(shape=(max_dec,), name='dec_input')\n",
        "dec_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d_full = dec_emb_layer_full(dec_input_full)\n",
        "# Add Reshape layer as used in training\n",
        "reshaped_emb_d_full = Reshape((max_dec, emb_dim))(emb_d_full)\n",
        "dec_lstm_full = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs_full, _, _ = dec_lstm_full(reshaped_emb_d_full, initial_state=[sh_full, sc_full])\n",
        "dec_dense_full = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits_full = dec_dense_full(dec_outs_full)\n",
        "\n",
        "# Full model for loading weights\n",
        "full_model = Model([enc_input_full, dec_input_full], logits_full)\n",
        "\n",
        "# Load trained weights into the full model\n",
        "full_model.load_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "print(\"Loaded model weights into full model from:\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "\n",
        "\n",
        "# Now define the encoder model for inference\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = full_model.get_layer('enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = full_model.get_layer('encoder_lstm')\n",
        "enc_outputs, state_h, state_c = enc_lstm(emb_e)\n",
        "encoder_model = Model(enc_input, [state_h, state_c])\n",
        "print(\"Defined encoder model for inference\")\n",
        "\n",
        "# Define the decoder model for inference\n",
        "dec_input = Input(shape=(1,), name='dec_input') # Decoder input is one token at a time\n",
        "dec_state_h = Input(shape=(units,), name='dec_state_h_input')\n",
        "dec_state_c = Input(shape=(units,), name='dec_state_c_input')\n",
        "dec_states_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "dec_emb_layer = full_model.get_layer('dec_embedding') # Reuse embedding layer\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "\n",
        "dec_lstm = full_model.get_layer('decoder_lstm') # Reuse LSTM layer\n",
        "dec_outputs, state_h_out, state_c_out = dec_lstm(emb_d, initial_state=dec_states_inputs)\n",
        "dec_states_outputs = [state_h_out, state_c_out]\n",
        "\n",
        "dec_dense = full_model.get_layer('decoder_dense') # Reuse dense layer\n",
        "output_tokens = dec_dense(dec_outputs)\n",
        "\n",
        "decoder_model = Model([dec_input] + dec_states_inputs, [output_tokens] + dec_states_outputs)\n",
        "print(\"Defined decoder model for inference\")\n",
        "\n",
        "\n",
        "# Define the inference function\n",
        "def generate_response(input_sentence, max_length=max_dec):\n",
        "    # Preprocess the input sentence\n",
        "    input_seq = sp.EncodeAsIds(\"<s> \" + input_sentence + \" </s>\")\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_enc, padding='post')\n",
        "\n",
        "    # Get the initial states from the encoder\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Start the decoder with the start token\n",
        "    start_token = sp.PieceToId(\"<s>\")\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token (greedy approach for simplicity)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Add a check to ensure the sampled index is within the valid range\n",
        "        if sampled_token_index < 0 or sampled_token_index >= V:\n",
        "            print(f\"Warning: Sampled token index {sampled_token_index} is out of vocabulary range [0, {V-1}]. Stopping generation.\")\n",
        "            break\n",
        "\n",
        "        # Explicitly cast to int just in case\n",
        "        sampled_token = sp.IdToPiece(int(sampled_token_index))\n",
        "\n",
        "        # Exit condition: hitting stop character or max length\n",
        "        if sampled_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        # Append token\n",
        "        decoded_sentence.append(sampled_token)\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Join the tokens to form the sentence, handling SentencePiece subwords\n",
        "    return \"\".join(decoded_sentence).replace(\" \", \" \").strip()\n",
        "\n",
        "# Test the inference function\n",
        "print(\"\\nTesting inference:\")\n",
        "sample_inputs = [\"क्या तुम मेरे साथ चलोगी?\", \"मुझे माफ कर दो\"]\n",
        "for sample_input in sample_inputs:\n",
        "    response = generate_response(sample_input)\n",
        "    print(f\"Input: {sample_input}\\nResponse: {response}\\n\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly.model\n",
            "SPM vocab size: 128\n",
            "Loaded model weights into full model from: /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "Defined encoder model for inference\n",
            "Defined decoder model for inference\n",
            "\n",
            "Testing inference:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_7' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:5 out of the last 35 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a860f2d93a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क्या तुम मेरे साथ चलोगी?\n",
            "Response: ▁रही▁रही▁रहीसा<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: मुझे माफ कर दो\n",
            "Response: ▁<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0fm1kl20Q5B",
        "outputId": "590893a1-3bcf-46d0-819b-9f6a88bb0d86"
      },
      "source": [
        "# Cell 4 - Inference - Retry\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix + \".model\")\n",
        "V = sp.GetPieceSize()\n",
        "print(\"Loaded SPM model from:\", sp_prefix + \".model\")\n",
        "print(\"SPM vocab size:\", V)\n",
        "\n",
        "# Model parameters (should match training)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "# Recreate the full model structure used during training to load weights\n",
        "# Encoder\n",
        "enc_input_full = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e_full = enc_emb_layer_full(enc_input_full)\n",
        "enc_lstm_full = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "enc_outputs_full, sh_full, sc_full = enc_lstm_full(emb_e_full)\n",
        "\n",
        "# Decoder\n",
        "dec_input_full = Input(shape=(max_dec,), name='dec_input')\n",
        "dec_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d_full = dec_emb_layer_full(dec_input_full)\n",
        "# Add Reshape layer as used in training\n",
        "reshaped_emb_d_full = Reshape((max_dec, emb_dim))(emb_d_full)\n",
        "dec_lstm_full = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs_full, _, _ = dec_lstm_full(reshaped_emb_d_full, initial_state=[sh_full, sc_full])\n",
        "dec_dense_full = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits_full = dec_dense_full(dec_outs_full)\n",
        "\n",
        "# Full model for loading weights\n",
        "full_model = Model([enc_input_full, dec_input_full], logits_full)\n",
        "\n",
        "# Load trained weights into the full model\n",
        "full_model.load_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "print(\"Loaded model weights into full model from:\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "\n",
        "\n",
        "# Now define the encoder model for inference\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = full_model.get_layer('enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = full_model.get_layer('encoder_lstm')\n",
        "enc_outputs, state_h, state_c = enc_lstm(emb_e)\n",
        "encoder_model = Model(enc_input, [state_h, state_c])\n",
        "print(\"Defined encoder model for inference\")\n",
        "\n",
        "# Define the decoder model for inference\n",
        "dec_input = Input(shape=(1,), name='dec_input') # Decoder input is one token at a time\n",
        "dec_state_h = Input(shape=(units,), name='dec_state_h_input')\n",
        "dec_state_c = Input(shape=(units,), name='dec_state_c_input')\n",
        "dec_states_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "dec_emb_layer = full_model.get_layer('dec_embedding') # Reuse embedding layer\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "\n",
        "dec_lstm = full_model.get_layer('decoder_lstm') # Reuse LSTM layer\n",
        "dec_outputs, state_h_out, state_c_out = dec_lstm(emb_d, initial_state=dec_states_inputs)\n",
        "dec_states_outputs = [state_h_out, state_c_out]\n",
        "\n",
        "dec_dense = full_model.get_layer('decoder_dense') # Reuse dense layer\n",
        "output_tokens = dec_dense(dec_outputs)\n",
        "\n",
        "decoder_model = Model([dec_input] + dec_states_inputs, [output_tokens] + dec_states_outputs)\n",
        "print(\"Defined decoder model for inference\")\n",
        "\n",
        "\n",
        "# Define the inference function\n",
        "def generate_response(input_sentence, max_length=max_dec):\n",
        "    # Preprocess the input sentence\n",
        "    input_seq = sp.EncodeAsIds(\"<s> \" + input_sentence + \" </s>\")\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_enc, padding='post')\n",
        "\n",
        "    # Get the initial states from the encoder\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Start the decoder with the start token\n",
        "    start_token = sp.PieceToId(\"<s>\")\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token (greedy approach for simplicity)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Add a check to ensure the sampled index is within the valid range\n",
        "        if sampled_token_index < 0 or sampled_token_index >= V:\n",
        "            print(f\"Warning: Sampled token index {sampled_token_index} is out of vocabulary range [0, {V-1}]. Stopping generation.\")\n",
        "            break\n",
        "\n",
        "        # Explicitly cast to int just in case\n",
        "        sampled_token = sp.IdToPiece(int(sampled_token_index))\n",
        "\n",
        "        # Exit condition: hitting stop character or max length\n",
        "        if sampled_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        # Append token\n",
        "        decoded_sentence.append(sampled_token)\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Join the tokens to form the sentence, handling SentencePiece subwords\n",
        "    return \"\".join(decoded_sentence).replace(\" \", \" \").strip()\n",
        "\n",
        "# Test the inference function\n",
        "print(\"\\nTesting inference:\")\n",
        "sample_inputs = [\"क्या तुम मेरे साथ चलोगी?\", \"मुझे माफ कर दो\"]\n",
        "for sample_input in sample_inputs:\n",
        "    response = generate_response(sample_input)\n",
        "    print(f\"Input: {sample_input}\\nResponse: {response}\\n\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly.model\n",
            "SPM vocab size: 128\n",
            "Loaded model weights into full model from: /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "Defined encoder model for inference\n",
            "Defined decoder model for inference\n",
            "\n",
            "Testing inference:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_8' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क्या तुम मेरे साथ चलोगी?\n",
            "Response: ▁रही▁रही▁रहीसा<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: मुझे माफ कर दो\n",
            "Response: ▁<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRwi6PoZ0Xh4",
        "outputId": "7bcc5d05-cf52-4319-8c07-4c1d28a21183"
      },
      "source": [
        "# Cell 4 - Inference - Retry 2: Address TypeError in sp.IdToPiece\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix + \".model\")\n",
        "V = sp.GetPieceSize()\n",
        "print(\"Loaded SPM model from:\", sp_prefix + \".model\")\n",
        "print(\"SPM vocab size:\", V)\n",
        "\n",
        "# Model parameters (should match training)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "# Recreate the full model structure used during training to load weights\n",
        "# Encoder\n",
        "enc_input_full = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e_full = enc_emb_layer_full(enc_input_full)\n",
        "enc_lstm_full = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "enc_outputs_full, sh_full, sc_full = enc_lstm_full(emb_e_full)\n",
        "\n",
        "# Decoder\n",
        "dec_input_full = Input(shape=(max_dec,), name='dec_input')\n",
        "dec_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d_full = dec_emb_layer_full(dec_input_full)\n",
        "# Add Reshape layer as used in training\n",
        "reshaped_emb_d_full = Reshape((max_dec, emb_dim))(emb_d_full)\n",
        "dec_lstm_full = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs_full, _, _ = dec_lstm_full(reshaped_emb_d_full, initial_state=[sh_full, sc_full])\n",
        "dec_dense_full = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits_full = dec_dense_full(dec_outs_full)\n",
        "\n",
        "# Full model for loading weights\n",
        "full_model = Model([enc_input_full, dec_input_full], logits_full)\n",
        "\n",
        "# Load trained weights into the full model\n",
        "full_model.load_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "print(\"Loaded model weights into full model from:\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "\n",
        "\n",
        "# Now define the encoder model for inference\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = full_model.get_layer('enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = full_model.get_layer('encoder_lstm')\n",
        "enc_outputs, state_h, state_c = enc_lstm(emb_e)\n",
        "encoder_model = Model(enc_input, [state_h, state_c])\n",
        "print(\"Defined encoder model for inference\")\n",
        "\n",
        "# Define the decoder model for inference\n",
        "dec_input = Input(shape=(1,), name='dec_input') # Decoder input is one token at a time\n",
        "dec_state_h = Input(shape=(units,), name='dec_state_h_input')\n",
        "dec_state_c = Input(shape=(units,), name='dec_state_c_input')\n",
        "dec_states_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "dec_emb_layer = full_model.get_layer('dec_embedding') # Reuse embedding layer\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "\n",
        "dec_lstm = full_model.get_layer('decoder_lstm') # Reuse LSTM layer\n",
        "dec_outputs, state_h_out, state_c_out = dec_lstm(emb_d, initial_state=dec_states_inputs)\n",
        "dec_states_outputs = [state_h_out, state_c_out]\n",
        "\n",
        "dec_dense = full_model.get_layer('decoder_dense') # Reuse dense layer\n",
        "output_tokens = dec_dense(dec_outputs)\n",
        "\n",
        "decoder_model = Model([dec_input] + dec_states_inputs, [output_tokens] + dec_states_outputs)\n",
        "print(\"Defined decoder model for inference\")\n",
        "\n",
        "\n",
        "# Define the inference function\n",
        "def generate_response(input_sentence, max_length=max_dec):\n",
        "    # Preprocess the input sentence\n",
        "    input_seq = sp.EncodeAsIds(\"<s> \" + input_sentence + \" </s>\")\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_enc, padding='post')\n",
        "\n",
        "    # Get the initial states from the encoder\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Start the decoder with the start token\n",
        "    start_token = sp.PieceToId(\"<s>\")\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token (greedy approach for simplicity)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Add a check to ensure the sampled index is within the valid range\n",
        "        if sampled_token_index < 0 or sampled_token_index >= V:\n",
        "            print(f\"Warning: Sampled token index {sampled_token_index} is out of vocabulary range [0, {V-1}]. Stopping generation.\")\n",
        "            break\n",
        "\n",
        "        # Explicitly cast to int just in case\n",
        "        sampled_token = sp.IdToPiece(int(sampled_token_index))\n",
        "\n",
        "        # Exit condition: hitting stop character or max length\n",
        "        if sampled_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        # Append token\n",
        "        decoded_sentence.append(sampled_token)\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Join the tokens to form the sentence, handling SentencePiece subwords\n",
        "    return \"\".join(decoded_sentence).replace(\" \", \" \").strip()\n",
        "\n",
        "# Test the inference function\n",
        "print(\"\\nTesting inference:\")\n",
        "sample_inputs = [\"क्या तुम मेरे साथ चलोगी?\", \"मुझे माफ कर दो\"]\n",
        "for sample_input in sample_inputs:\n",
        "    response = generate_response(sample_input)\n",
        "    print(f\"Input: {sample_input}\\nResponse: {response}\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly.model\n",
            "SPM vocab size: 128\n",
            "Loaded model weights into full model from: /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "Defined encoder model for inference\n",
            "Defined decoder model for inference\n",
            "\n",
            "Testing inference:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_5' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क्या तुम मेरे साथ चलोगी?\n",
            "Response: ▁रही▁रही▁रहीसा<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: मुझे माफ कर दो\n",
            "Response: ▁<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a96b80d8",
        "outputId": "0a9ec095-bdc2-438e-a300-cb68d36609dd"
      },
      "source": [
        "# Cell 5 - Flask App\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "\n",
        "# Assuming generate_response and sp are available from the previous cell\n",
        "# from __main__ import generate_response, sp # This is not reliable in all notebook environments\n",
        "\n",
        "# Define the Flask application instance\n",
        "app = Flask(__name__)\n",
        "\n",
        "# HTML template for the home page\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!doctype html>\n",
        "<html>\n",
        "<head><title>Bolly Chatbot</title></head>\n",
        "<body>\n",
        "    <h1>Bolly Chatbot</h1>\n",
        "    <form id=\"chat-form\">\n",
        "        <input type=\"text\" id=\"user-input\" placeholder=\"Enter your message\">\n",
        "        <button type=\"submit\">Send</button>\n",
        "    </form>\n",
        "    <div id=\"chat-output\"></div>\n",
        "\n",
        "    <script>\n",
        "        document.getElementById('chat-form').onsubmit = async function(event) {\n",
        "            event.preventDefault();\n",
        "            const userInput = document.getElementById('user-input').value;\n",
        "            const chatOutput = document.getElementById('chat-output');\n",
        "\n",
        "            // Display user input\n",
        "            chatOutput.innerHTML += `<p><strong>You:</strong> ${userInput}</p>`;\n",
        "\n",
        "            // Clear input field\n",
        "            document.getElementById('user-input').value = '';\n",
        "\n",
        "            // Send message to backend\n",
        "            const response = await fetch('/chat', {\n",
        "                method: 'POST',\n",
        "                headers: {\n",
        "                    'Content-Type': 'application/json'\n",
        "                },\n",
        "                body: JSON.stringify({ message: userInput })\n",
        "            });\n",
        "\n",
        "            const data = await response.json();\n",
        "\n",
        "            // Display bot response\n",
        "            chatOutput.innerHTML += `<p><strong>Bot:</strong> ${data.response}</p>`;\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_message = request.json.get('message')\n",
        "    if not user_message:\n",
        "        return jsonify({\"response\": \"Error: No message received.\"}), 400\n",
        "\n",
        "    # Call the generate_response function\n",
        "    # This function needs to be accessible in this scope.\n",
        "    # In a real app, you might pass it or import it properly.\n",
        "    # For this notebook context, we assume it's in the global scope.\n",
        "    try:\n",
        "        bot_response = generate_response(user_message)\n",
        "    except Exception as e:\n",
        "        # Log the error for debugging\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        bot_response = \"Sorry, I am unable to respond at the moment.\"\n",
        "\n",
        "    return jsonify({\"response\": bot_response})\n",
        "\n",
        "# To run the app in a notebook, you typically use ngrok or similar.\n",
        "# We'll add the ngrok part in a separate cell to make it easier to stop and restart.\n",
        "\n",
        "print(\"Flask app defined. Use ngrok in the next cell to expose it.\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app defined. Use ngrok in the next cell to expose it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eafb2f8",
        "outputId": "d962007e-f0af-47c8-8851-e081bd848c47"
      },
      "source": [
        "# Cell 6 - Expose Flask app with ngrok\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Terminate any previous ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Define a function to run the Flask app\n",
        "def run_flask_app():\n",
        "    # Use app.run() with debug=False for ngrok\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "\n",
        "# Run the Flask app in a separate thread\n",
        "thread = threading.Thread(target=run_flask_app)\n",
        "thread.start()\n",
        "print(\"Flask app is running in a separate thread.\")\n",
        "\n",
        "# Give the Flask app a moment to start\n",
        "time.sleep(2)\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "try:\n",
        "    # Connect to the Flask port\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(f\" * ngrok tunnel is live at: {public_url}\")\n",
        "    print(\" * You can access the chatbot via this URL.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok: {e}\")\n",
        "    print(\"Could not establish ngrok tunnel. The Flask app might not be running or port 5000 is in use.\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app is running in a separate thread.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-08-14T09:03:58+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-08-14T09:03:58+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-08-14T09:03:58+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error starting ngrok: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.\n",
            "Could not establish ngrok tunnel. The Flask app might not be running or port 5000 is in use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Lm_ebg0oXJ",
        "outputId": "0798e80b-f321-4c47-9659-d425847564b8"
      },
      "source": [
        "# Cell 7 - Configure ngrok authtoken and expose Flask app\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Terminate any previous ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Get ngrok authtoken from environment variables or configure it directly\n",
        "# It's recommended to set this as an environment variable in a real application.\n",
        "# For Colab, you can add it in a secrets manager or directly here for demonstration.\n",
        "# Replace 'YOUR_NGROK_AUTHTOKEN' with your actual authtoken if not using env var\n",
        "NGROK_AUTH_TOKEN = os.environ.get(\"NGROK_AUTH_TOKEN\", \"YOUR_NGROK_AUTHTOKEN\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTHTOKEN\":\n",
        "    print(\"WARNING: ngrok authtoken is not set. Please replace 'YOUR_NGROK_AUTHTOKEN' with your actual token or set the NGROK_AUTH_TOKEN environment variable.\")\n",
        "    # Attempt to configure even if it's the placeholder, ngrok will likely fail but show the error again.\n",
        "    try:\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        print(\"Attempted to set ngrok authtoken.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting ngrok authtoken: {e}\")\n",
        "else:\n",
        "    try:\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        print(\"ngrok authtoken configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting ngrok authtoken: {e}\")\n",
        "\n",
        "\n",
        "# Define a function to run the Flask app\n",
        "def run_flask_app():\n",
        "    # Use app.run() with debug=False for ngrok\n",
        "    # Ensure 'app' is accessible, assuming it's defined in a previous cell's global scope\n",
        "    global app\n",
        "    if 'app' not in globals():\n",
        "        print(\"Error: Flask app 'app' not found. Please ensure the cell defining the Flask app is run first.\")\n",
        "        return\n",
        "    try:\n",
        "        app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error running Flask app: {e}\")\n",
        "\n",
        "\n",
        "# Run the Flask app in a separate thread only if it's not already running\n",
        "# Check if the thread exists and is alive to prevent starting multiple times\n",
        "if 'thread' not in globals() or not thread.is_alive():\n",
        "    thread = threading.Thread(target=run_flask_app)\n",
        "    thread.daemon = True # Allow the main program to exit even if the thread is running\n",
        "    thread.start()\n",
        "    print(\"Flask app is running in a separate thread.\")\n",
        "else:\n",
        "    print(\"Flask app thread is already running.\")\n",
        "\n",
        "# Give the Flask app a moment to start\n",
        "time.sleep(2)\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "try:\n",
        "    # Connect to the Flask port\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(f\" * ngrok tunnel is live at: {public_url}\")\n",
        "    print(\" * You can access the chatbot via this URL.\")\n",
        "    print(\"\\nNOTE: Keep this cell running to keep the ngrok tunnel active. To stop, interrupt the kernel.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok tunnel: {e}\")\n",
        "    print(\"Could not establish ngrok tunnel. Ensure your authtoken is correct and the Flask app is running.\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: ngrok authtoken is not set. Please replace 'YOUR_NGROK_AUTHTOKEN' with your actual token or set the NGROK_AUTH_TOKEN environment variable.\n",
            "Attempted to set ngrok authtoken.\n",
            "Flask app thread is already running.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-08-14T09:04:24+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: The authtoken you specified is an ngrok v1 authtoken, but you're using ngrok v2.\\nYour authtoken: YOUR_NGROK_AUTHTOKEN\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_106\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error starting ngrok tunnel: The ngrok process errored on start: authentication failed: The authtoken you specified is an ngrok v1 authtoken, but you're using ngrok v2.\\nYour authtoken: YOUR_NGROK_AUTHTOKEN\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_106\\r\\n.\n",
            "Could not establish ngrok tunnel. Ensure your authtoken is correct and the Flask app is running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c35e334c",
        "outputId": "92c24c2e-0ef6-44ea-cbd1-de0663941500"
      },
      "source": [
        "# Cell 7 - Configure ngrok authtoken and expose Flask app - Retry with Authtoken guidance\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Terminate any previous ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# --- IMPORTANT: Replace 'YOUR_NGROK_AUTHTOKEN' with your actual ngrok authtoken ---\n",
        "# 1. Go to ngrok.com and sign up or log in.\n",
        "# 2. Go to the 'Your Authtoken' page (usually under Setup & Installation).\n",
        "# 3. Copy your authtoken.\n",
        "# 4. Paste your authtoken below, replacing 'YOUR_NGROK_AUTHTOKEN'.\n",
        "NGROK_AUTH_TOKEN = \"31FKMpD0BYx5HaH142dOEAZOGuN_75D6PanqqWVcxqzergDGU\" # <--- PASTE YOUR AUTHTOKEN HERE\n",
        "\n",
        "# Alternatively, you can set the NGROK_AUTH_TOKEN environment variable\n",
        "# NGROK_AUTH_TOKEN = os.environ.get(\"NGROK_AUTH_TOKEN\", \"YOUR_NGROK_AUTHTOKEN\")\n",
        "# print(f\"Using NGROK_AUTH_TOKEN: {NGROK_AUTH_TOKEN}\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTHTOKEN\":\n",
        "    print(\"ERROR: ngrok authtoken is NOT set. Please replace 'YOUR_NGROK_AUTHTOKEN' with your actual token from ngrok.com.\")\n",
        "    # Do not attempt to configure or connect if the placeholder is still there.\n",
        "else:\n",
        "    try:\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        print(\"ngrok authtoken configured.\")\n",
        "\n",
        "        # Define a function to run the Flask app\n",
        "        def run_flask_app():\n",
        "            # Use app.run() with debug=False for ngrok\n",
        "            # Ensure 'app' is accessible, assuming it's defined in a previous cell's global scope\n",
        "            global app\n",
        "            if 'app' not in globals():\n",
        "                print(\"Error: Flask app 'app' not found. Please ensure the cell defining the Flask app is run first.\")\n",
        "                return\n",
        "            try:\n",
        "                # Suppress Flask's default output when running with debug=False and ngrok\n",
        "                import logging\n",
        "                log = logging.getLogger('werkzeug')\n",
        "                log.setLevel(logging.ERROR)\n",
        "                app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Error running Flask app: {e}\")\n",
        "\n",
        "\n",
        "        # Run the Flask app in a separate thread only if it's not already running\n",
        "        # Check if the thread exists and is alive to prevent starting multiple times\n",
        "        if 'thread' not in globals() or not thread.is_alive():\n",
        "            print(\"Starting Flask app in a separate thread...\")\n",
        "            thread = threading.Thread(target=run_flask_app)\n",
        "            thread.daemon = True # Allow the main program to exit even if the thread is running\n",
        "            thread.start()\n",
        "            print(\"Flask app thread started.\")\n",
        "\n",
        "            # Give the Flask app a moment to start\n",
        "            time.sleep(3) # Increased sleep time\n",
        "\n",
        "        else:\n",
        "            print(\"Flask app thread is already running.\")\n",
        "\n",
        "\n",
        "        # Set up ngrok tunnel\n",
        "        print(\"Attempting to establish ngrok tunnel...\")\n",
        "        try:\n",
        "            # Connect to the Flask port\n",
        "            public_url = ngrok.connect(5000).public_url\n",
        "            print(f\" * ngrok tunnel is live at: {public_url}\")\n",
        "            print(\" * You can access the chatbot via this URL.\")\n",
        "            print(\"\\nNOTE: Keep this cell running to keep the ngrok tunnel active. To stop, interrupt the kernel.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error starting ngrok tunnel: {e}\")\n",
        "            print(\"Could not establish ngrok tunnel. Ensure your authtoken is correct and the Flask app is running.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during ngrok setup: {e}\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken configured.\n",
            "Starting Flask app in a separate thread...\n",
            "Flask app thread started.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to establish ngrok tunnel...\n",
            " * ngrok tunnel is live at: https://5fb0b932526e.ngrok-free.app\n",
            " * You can access the chatbot via this URL.\n",
            "\n",
            "NOTE: Keep this cell running to keep the ngrok tunnel active. To stop, interrupt the kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d14c6c5b",
        "outputId": "3cade250-8f70-4311-d17f-ccd32c7fa508"
      },
      "source": [
        "# Cell 7 - Configure ngrok authtoken and expose Flask app - Retry with Authtoken guidance\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Terminate any previous ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# --- IMPORTANT: Replace 'YOUR_NGROK_AUTHTOKEN' with your actual ngrok authtoken ---\n",
        "# 1. Go to ngrok.com and sign up or log in.\n",
        "# 2. Go to the 'Your Authtoken' page (usually under Setup & Installation).\n",
        "# 3. Copy your authtoken.\n",
        "# 4. Paste your authtoken below, replacing 'YOUR_NGROK_AUTHTOKEN'.\n",
        "NGROK_AUTH_TOKEN = \"31FKMpD0BYx5HaH142dOEAZOGuN_75D6PanqqWVcxqzergDGU\" # <--- PASTE YOUR AUTHTOKEN HERE\n",
        "\n",
        "# Alternatively, you can set the NGROK_AUTH_TOKEN environment variable\n",
        "# NGROK_AUTH_TOKEN = os.environ.get(\"NGROK_AUTH_TOKEN\", \"YOUR_NGROK_AUTHTOKEN\")\n",
        "# print(f\"Using NGROK_AUTH_TOKEN: {NGROK_AUTH_TOKEN}\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTHTOKEN\":\n",
        "    print(\"ERROR: ngrok authtoken is NOT set. Please replace 'YOUR_NGROK_AUTHTOKEN' with your actual token from ngrok.com.\")\n",
        "    # Do not attempt to configure or connect if the placeholder is still there.\n",
        "else:\n",
        "    try:\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        print(\"ngrok authtoken configured.\")\n",
        "\n",
        "        # Define a function to run the Flask app\n",
        "        def run_flask_app():\n",
        "            # Use app.run() with debug=False for ngrok\n",
        "            # Ensure 'app' is accessible, assuming it's defined in a previous cell's global scope\n",
        "            global app\n",
        "            if 'app' not in globals():\n",
        "                print(\"Error: Flask app 'app' not found. Please ensure the cell defining the Flask app is run first.\")\n",
        "                return\n",
        "            try:\n",
        "                # Suppress Flask's default output when running with debug=False and ngrok\n",
        "                import logging\n",
        "                log = logging.getLogger('werkzeug')\n",
        "                log.setLevel(logging.ERROR)\n",
        "                app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Error running Flask app: {e}\")\n",
        "\n",
        "\n",
        "        # Run the Flask app in a separate thread only if it's not already running\n",
        "        # Check if the thread exists and is alive to prevent starting multiple times\n",
        "        if 'thread' not in globals() or not thread.is_alive():\n",
        "            print(\"Starting Flask app in a separate thread...\")\n",
        "            thread = threading.Thread(target=run_flask_app)\n",
        "            thread.daemon = True # Allow the main program to exit even if the thread is running\n",
        "            thread.start()\n",
        "            print(\"Flask app thread started.\")\n",
        "\n",
        "            # Give the Flask app a moment to start\n",
        "            time.sleep(3) # Increased sleep time\n",
        "\n",
        "        else:\n",
        "            print(\"Flask app thread is already running.\")\n",
        "\n",
        "\n",
        "        # Set up ngrok tunnel\n",
        "        print(\"Attempting to establish ngrok tunnel...\")\n",
        "        try:\n",
        "            # Connect to the Flask port\n",
        "            public_url = ngrok.connect(5000).public_url\n",
        "            print(f\" * ngrok tunnel is live at: {public_url}\")\n",
        "            print(\" * You can access the chatbot via this URL.\")\n",
        "            print(\"\\nNOTE: Keep this cell running to keep the ngrok tunnel active. To stop, interrupt the kernel.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error starting ngrok tunnel: {e}\")\n",
        "            print(\"Could not establish ngrok tunnel. Ensure your authtoken is correct and the Flask app is running.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during ngrok setup: {e}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken configured.\n",
            "Starting Flask app in a separate thread...\n",
            "Flask app thread started.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to establish ngrok tunnel...\n",
            " * ngrok tunnel is live at: https://70438a06a5f4.ngrok-free.app\n",
            " * You can access the chatbot via this URL.\n",
            "\n",
            "NOTE: Keep this cell running to keep the ngrok tunnel active. To stop, interrupt the kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15a5fddf",
        "outputId": "d954c84c-58a1-420a-b5e9-e34dac71861a"
      },
      "source": [
        "# Cell 5 - Flask App\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "\n",
        "# Assuming generate_response and sp are available from the previous cell\n",
        "# from __main__ import generate_response, sp # This is not reliable in all notebook environments\n",
        "\n",
        "# Define the Flask application instance\n",
        "app = Flask(__name__)\n",
        "\n",
        "# HTML template for the home page\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!doctype html>\n",
        "<html>\n",
        "<head><title>Bolly Chatbot</title></head>\n",
        "<body>\n",
        "    <h1>Bolly Chatbot</h1>\n",
        "    <form id=\"chat-form\">\n",
        "        <input type=\"text\" id=\"user-input\" placeholder=\"Enter your message\">\n",
        "        <button type=\"submit\">Send</button>\n",
        "    </form>\n",
        "    <div id=\"chat-output\"></div>\n",
        "\n",
        "    <script>\n",
        "        document.getElementById('chat-form').onsubmit = async function(event) {\n",
        "            event.preventDefault();\n",
        "            const userInput = document.getElementById('user-input').value;\n",
        "            const chatOutput = document.getElementById('chat-output');\n",
        "\n",
        "            // Display user input\n",
        "            chatOutput.innerHTML += `<p><strong>You:</strong> ${userInput}</p>`;\n",
        "\n",
        "            // Clear input field\n",
        "            document.getElementById('user-input').value = '';\n",
        "\n",
        "            // Send message to backend\n",
        "            const response = await fetch('/chat', {\n",
        "                method: 'POST',\n",
        "                headers: {\n",
        "                    'Content-Type': 'application/json'\n",
        "                },\n",
        "                body: JSON.stringify({ message: userInput })\n",
        "            });\n",
        "\n",
        "            const data = await response.json();\n",
        "\n",
        "            // Display bot response\n",
        "            chatOutput.innerHTML += `<p><strong>Bot:</strong> ${data.response}</p>`;\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_message = request.json.get('message')\n",
        "    if not user_message:\n",
        "        return jsonify({\"response\": \"Error: No message received.\"}), 400\n",
        "\n",
        "    # Call the generate_response function\n",
        "    # This function needs to be accessible in this scope.\n",
        "    # In a real app, you might pass it or import it properly.\n",
        "    # For this notebook context, we assume it's in the global scope.\n",
        "    try:\n",
        "        bot_response = generate_response(user_message)\n",
        "    except Exception as e:\n",
        "        # Log the error for debugging\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        bot_response = \"Sorry, I am unable to respond at the moment.\"\n",
        "\n",
        "    return jsonify({\"response\": bot_response})\n",
        "\n",
        "# To run the app in a notebook, you typically use ngrok or similar.\n",
        "# We'll add the ngrok part in a separate cell to make it easier to stop and restart.\n",
        "\n",
        "print(\"Flask app defined. Use ngrok in the next cell to expose it.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app defined. Use ngrok in the next cell to expose it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "788eee8e",
        "outputId": "bcf860e6-7144-44bb-bd93-1843b27790ae"
      },
      "source": [
        "# Cell 6 - Expose Flask app with ngrok\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Terminate any previous ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Define a function to run the Flask app\n",
        "def run_flask_app():\n",
        "    # Use app.run() with debug=False for ngrok\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "\n",
        "# Run the Flask app in a separate thread\n",
        "thread = threading.Thread(target=run_flask_app)\n",
        "thread.start()\n",
        "print(\"Flask app is running in a separate thread.\")\n",
        "\n",
        "# Give the Flask app a moment to start\n",
        "time.sleep(2)\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "try:\n",
        "    # Connect to the Flask port\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(f\" * ngrok tunnel is live at: {public_url}\")\n",
        "    print(\" * You can access the chatbot via this URL.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok: {e}\")\n",
        "    print(\"Could not establish ngrok tunnel. The Flask app might not be running or port 5000 is in use.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n",
            "Flask app is running in a separate thread.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n",
            "ERROR:pyngrok.process.ngrok:t=2025-08-14T09:08:45+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: The authtoken you specified is an ngrok v1 authtoken, but you're using ngrok v2.\\nYour authtoken: YOUR_NGROK_AUTHTOKEN\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_106\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-08-14T09:08:45+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: The authtoken you specified is an ngrok v1 authtoken, but you're using ngrok v2.\\nYour authtoken: YOUR_NGROK_AUTHTOKEN\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_106\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-08-14T09:08:45+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: The authtoken you specified is an ngrok v1 authtoken, but you're using ngrok v2.\\nYour authtoken: YOUR_NGROK_AUTHTOKEN\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_106\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error starting ngrok: The ngrok process errored on start: authentication failed: The authtoken you specified is an ngrok v1 authtoken, but you're using ngrok v2.\\nYour authtoken: YOUR_NGROK_AUTHTOKEN\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_106\\r\\n.\n",
            "Could not establish ngrok tunnel. The Flask app might not be running or port 5000 is in use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b48bd236",
        "outputId": "59d44bd7-3703-41c9-cf0e-ccc6471f58b6"
      },
      "source": [
        "# Cell 2 - Collect and prepare data - Update for new dataset\n",
        "\n",
        "# Assuming the new dataset is in the same /content/bolly_chatbot/data/ directory\n",
        "# and might include multiple .srt files or similar text files.\n",
        "# Modify the glob pattern if the new files have a different extension.\n",
        "\n",
        "lines = []\n",
        "# Updated glob pattern to potentially include more files\n",
        "for p in DATA.glob(\"*.srt\"): # Adjust pattern if necessary, e.g., \"*.txt\" or \"*.*\"\n",
        "    print(f\"Parsing file: {p}\")\n",
        "    lines += parse_srt_to_lines(p)\n",
        "\n",
        "print(\"Lines found:\", len(lines))\n",
        "\n",
        "# Build pairs from the collected lines\n",
        "pairs = build_pairs_from_lines(lines)\n",
        "print(\"Pairs:\", len(pairs))\n",
        "\n",
        "if len(pairs) == 0:\n",
        "    raise SystemExit(\"No pairs found. Ensure the new dataset files are in /content/bolly_chatbot/data/ and have the correct file extension.\")\n",
        "\n",
        "# Write the updated corpus\n",
        "with open(CORPUS, \"w\", encoding=\"utf-8\") as f:\n",
        "    for a, b in pairs:\n",
        "        f.write(a + \"\\n\")\n",
        "        f.write(b + \"\\n\")\n",
        "print(\"Wrote updated corpus:\", CORPUS)\n",
        "\n",
        "# Display the first few pairs to verify\n",
        "print(\"\\nSample pairs from the new dataset:\")\n",
        "for i, pair in enumerate(pairs[:5]):\n",
        "    print(f\"Pair {i+1}: {pair}\")\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing file: /content/bolly_chatbot/data/sample_subs.srt\n",
            "Lines found: 3\n",
            "Pairs: 2\n",
            "Wrote updated corpus: /content/bolly_chatbot/data/corpus.txt\n",
            "\n",
            "Sample pairs from the new dataset:\n",
            "Pair 1: ('क्या तुम मेरे साथ चलोगी?', 'क्यों नहीं, मैं आ रही हूँ।')\n",
            "Pair 2: ('क्यों नहीं, मैं आ रही हूँ।', 'मुझे माफ कर दो, मेरी गलती थी।')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5c9a5006",
        "outputId": "17675712-cda5-4b3f-a1ba-5bebd685568f"
      },
      "source": [
        "# Cell 3 - Train small SentencePiece and tiny seq2seq (fast) - Retry with updated data\n",
        "\n",
        "# Ensure SentencePiece model is trained only if it doesn't exist or force retraining\n",
        "# For retraining with new data, we should overwrite the existing model\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "spm.SentencePieceTrainer.Train(f\"--input={CORPUS} --model_prefix={sp_prefix} --vocab_size=128 --character_coverage=0.9995 --model_type=bpe --user_defined_symbols=<s>,</s>\")\n",
        "print(\"Trained SPM:\", sp_prefix + \".model\")\n",
        "\n",
        "\n",
        "# prepare encoded data\n",
        "sp = spm.SentencePieceProcessor(); sp.Load(sp_prefix + \".model\")\n",
        "V = sp.GetPieceSize() # Update vocabulary size based on the newly trained model\n",
        "print(\"SPM vocab size:\", V)\n",
        "\n",
        "def encode(s, maxlen=16):\n",
        "    txt = \"<s> \" + s + \" </s>\"\n",
        "    encoded_ids = sp.EncodeAsIds(txt)\n",
        "    # Ensure truncation happens if needed\n",
        "    return encoded_ids[:maxlen]\n",
        "\n",
        "max_enc=16; max_dec=15\n",
        "encs=[]; decins=[]; decouts=[]\n",
        "for a,b in pairs:\n",
        "    e = encode(a,max_enc)\n",
        "    d = encode(b,max_dec+1)\n",
        "    if len(d) < 2: continue # Skip empty or single-token decoder sequences\n",
        "    encs.append(e)\n",
        "    decins.append(d[:-1])\n",
        "    decouts.append(d[1:])\n",
        "\n",
        "# Convert to numpy arrays and pad\n",
        "# Ensure there are sequences to process before padding\n",
        "if not encs:\n",
        "    raise SystemExit(\"No valid sequences generated from pairs. Check data and encoding.\")\n",
        "\n",
        "encs = pad_sequences(encs, maxlen=max_enc, padding='post')\n",
        "decins = pad_sequences(decins, maxlen=max_dec, padding='post')\n",
        "decouts = pad_sequences(decouts, maxlen=max_dec, padding='post')\n",
        "decouts = np.expand_dims(decouts, -1) # Add a dimension for sparse_categorical_crossentropy\n",
        "\n",
        "print(f\"Prepared {len(encs)} encoded sequences.\")\n",
        "print(\"Encoder shape:\", encs.shape)\n",
        "print(\"Decoder input shape:\", decins.shape)\n",
        "print(\"Decoder output shape:\", decouts.shape)\n",
        "\n",
        "\n",
        "# define tiny seq2seq model\n",
        "# Reuse model definition from previous attempts, ensuring layer names match\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "emb_dim=48; units=48 # Use the same dimensions as before\n",
        "\n",
        "# Encoder\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "# Use the updated vocabulary size V\n",
        "enc_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "_, sh, sc = enc_lstm(emb_e) # Don't need the full sequence output from encoder LSTM\n",
        "\n",
        "# Decoder\n",
        "dec_input = Input(shape=(max_dec,), name='dec_input')\n",
        "# Use the updated vocabulary size V\n",
        "dec_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "# Add Reshape layer to explicitly set shape if needed (matching training structure)\n",
        "# Check if this Reshape was truly necessary or caused warnings/errors before.\n",
        "# Based on previous runs, the warning about mask loss suggests it might be\n",
        "# better to remove if possible, but let's keep it for now to match the structure\n",
        "# that successfully loaded weights previously.\n",
        "# If the model doesn't train or predict correctly, this might be a place to revisit.\n",
        "# Let's add the Reshape back as it was in the successful inference attempts.\n",
        "reshaped_emb_d = Reshape((max_dec, emb_dim))(emb_d)\n",
        "\n",
        "dec_lstm = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "# Initial state should be from the encoder\n",
        "dec_outs, _, _ = dec_lstm(reshaped_emb_d, initial_state=[sh, sc])\n",
        "# Use the updated vocabulary size V\n",
        "dec_dense = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits = dec_dense(dec_outs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([enc_input, dec_input], logits)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Print model summary to verify structure\n",
        "model.summary()\n",
        "\n",
        "\n",
        "print(\"Training tiny model...\")\n",
        "# Train the model on the new data\n",
        "# Adjust epochs and batch size if needed based on dataset size and performance\n",
        "model.fit([encs, decins], decouts, batch_size=4, epochs=10, verbose=2) # Increased epochs slightly\n",
        "\n",
        "\n",
        "# save artifacts\n",
        "# Save weights after successful training\n",
        "model.save_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "# SentencePiece model is already saved during the SentencePieceTrainer.Train call\n",
        "print(\"Saved weights to\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "print(\"SPM vocab size:\", sp.GetPieceSize())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained SPM: /content/bolly_chatbot/models/spm_bolly.model\n",
            "SPM vocab size: 128\n",
            "Prepared 2 encoded sequences.\n",
            "Encoder shape: (2, 16)\n",
            "Decoder input shape: (2, 15)\n",
            "Decoder output shape: (2, 15, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_9' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_26\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_26\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ dec_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m48\u001b[0m)    │      \u001b[38;5;34m6,144\u001b[0m │ dec_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m48\u001b[0m)    │      \u001b[38;5;34m6,144\u001b[0m │ enc_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_40        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ enc_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_9 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m48\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dec_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m),      │     \u001b[38;5;34m18,624\u001b[0m │ enc_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m),       │            │ not_equal_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m48\u001b[0m),  │     \u001b[38;5;34m18,624\u001b[0m │ reshape_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m),       │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)]       │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m6,272\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ dec_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │ dec_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │ enc_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_40        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enc_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,624</span> │ enc_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),       │            │ not_equal_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,624</span> │ reshape_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),       │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)]       │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,808\u001b[0m (218.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,808</span> (218.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,808\u001b[0m (218.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,808</span> (218.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tiny model...\n",
            "Epoch 1/10\n",
            "1/1 - 5s - 5s/step - loss: 4.8486\n",
            "Epoch 2/10\n",
            "1/1 - 0s - 60ms/step - loss: 4.8419\n",
            "Epoch 3/10\n",
            "1/1 - 0s - 61ms/step - loss: 4.8350\n",
            "Epoch 4/10\n",
            "1/1 - 0s - 61ms/step - loss: 4.8279\n",
            "Epoch 5/10\n",
            "1/1 - 0s - 59ms/step - loss: 4.8204\n",
            "Epoch 6/10\n",
            "1/1 - 0s - 73ms/step - loss: 4.8122\n",
            "Epoch 7/10\n",
            "1/1 - 0s - 67ms/step - loss: 4.8033\n",
            "Epoch 8/10\n",
            "1/1 - 0s - 59ms/step - loss: 4.7935\n",
            "Epoch 9/10\n",
            "1/1 - 0s - 59ms/step - loss: 4.7825\n",
            "Epoch 10/10\n",
            "1/1 - 0s - 59ms/step - loss: 4.7700\n",
            "Saved weights to /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "SPM vocab size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3xn88VY3m-s",
        "outputId": "859ede0a-17e7-4926-f8e7-30b565f69dcf"
      },
      "source": [
        "# Cell 4 - Inference - Test with retrained model\n",
        "# Reusing the inference code from previous attempts\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix + \".model\")\n",
        "V = sp.GetPieceSize()\n",
        "print(\"Loaded SPM model from:\", sp_prefix + \".model\")\n",
        "print(\"SPM vocab size:\", V)\n",
        "\n",
        "# Model parameters (should match training)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "# Recreate the full model structure used during training to load weights\n",
        "# Ensure layer names match exactly\n",
        "# Encoder\n",
        "enc_input_full = Input(shape=(max_enc,), name='enc_input')\n",
        "# Use the updated vocabulary size V\n",
        "enc_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e_full = enc_emb_layer_full(enc_input_full)\n",
        "enc_lstm_full = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "enc_outputs_full, sh_full, sc_full = enc_lstm_full(emb_e_full)\n",
        "\n",
        "# Decoder\n",
        "dec_input_full = Input(shape=(max_dec,), name='dec_input')\n",
        "# Use the updated vocabulary size V\n",
        "dec_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d_full = dec_emb_layer_full(dec_input_full)\n",
        "# Add Reshape layer as used in training\n",
        "reshaped_emb_d_full = Reshape((max_dec, emb_dim))(emb_d_full)\n",
        "dec_lstm_full = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs_full, _, _ = dec_lstm_full(reshaped_emb_d_full, initial_state=[sh_full, sc_full])\n",
        "dec_dense_full = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits_full = dec_dense_full(dec_outs_full)\n",
        "\n",
        "# Full model for loading weights\n",
        "full_model = Model([enc_input_full, dec_input_full], logits_full)\n",
        "\n",
        "# Load trained weights into the full model\n",
        "full_model.load_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "print(\"Loaded model weights into full model from:\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "\n",
        "\n",
        "# Now define the encoder model for inference\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = full_model.get_layer('enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = full_model.get_layer('encoder_lstm')\n",
        "enc_outputs, state_h, state_c = enc_lstm(emb_e)\n",
        "encoder_model = Model(enc_input, [state_h, state_c])\n",
        "print(\"Defined encoder model for inference\")\n",
        "\n",
        "# Define the decoder model for inference\n",
        "dec_input = Input(shape=(1,), name='dec_input') # Decoder input is one token at a time\n",
        "dec_state_h = Input(shape=(units,), name='dec_state_h_input')\n",
        "dec_state_c = Input(shape=(units,), name='dec_state_c_input')\n",
        "dec_states_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "dec_emb_layer = full_model.get_layer('dec_embedding') # Reuse embedding layer\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "\n",
        "dec_lstm = full_model.get_layer('decoder_lstm') # Reuse LSTM layer\n",
        "dec_outputs, state_h_out, state_c_out = dec_lstm(emb_d, initial_state=dec_states_inputs)\n",
        "dec_states_outputs = [state_h_out, state_c_out]\n",
        "\n",
        "dec_dense = full_model.get_layer('decoder_dense') # Reuse dense layer\n",
        "output_tokens = dec_dense(dec_outputs)\n",
        "\n",
        "decoder_model = Model([dec_input] + dec_states_inputs, [output_tokens] + dec_states_outputs)\n",
        "print(\"Defined decoder model for inference\")\n",
        "\n",
        "\n",
        "# Define the inference function\n",
        "def generate_response(input_sentence, max_length=max_dec):\n",
        "    # Preprocess the input sentence\n",
        "    input_seq = sp.EncodeAsIds(\"<s> \" + input_sentence + \" </s>\")\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_enc, padding='post')\n",
        "\n",
        "    # Get the initial states from the encoder\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Start the decoder with the start token\n",
        "    start_token = sp.PieceToId(\"<s>\")\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token (greedy approach for simplicity)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Add a check to ensure the sampled index is within the valid range\n",
        "        if sampled_token_index < 0 or sampled_token_index >= V:\n",
        "            print(f\"Warning: Sampled token index {sampled_token_index} is out of vocabulary range [0, {V-1}]. Stopping generation.\")\n",
        "            break\n",
        "\n",
        "        # Explicitly cast to int just in case\n",
        "        sampled_token = sp.IdToPiece(int(sampled_token_index))\n",
        "\n",
        "        # Exit condition: hitting stop character or max length\n",
        "        if sampled_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        # Append token\n",
        "        decoded_sentence.append(sampled_token)\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Join the tokens to form the sentence, handling SentencePiece subwords\n",
        "    return \"\".join(decoded_sentence).replace(\" \", \" \").strip()\n",
        "\n",
        "# Test the inference function\n",
        "print(\"\\nTesting inference:\")\n",
        "# Use sample inputs relevant to the (potentially new) dataset\n",
        "sample_inputs = [\"क्या तुम मेरे साथ चलोगी?\", \"मुझे माफ कर दो\", \"hello\", \"how are you?\"]\n",
        "for sample_input in sample_inputs:\n",
        "    response = generate_response(sample_input)\n",
        "    print(f\"Input: {sample_input}\\nResponse: {response}\\n\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly.model\n",
            "SPM vocab size: 128\n",
            "Loaded model weights into full model from: /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "Defined encoder model for inference\n",
            "Defined decoder model for inference\n",
            "\n",
            "Testing inference:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_10' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क्या तुम मेरे साथ चलोगी?\n",
            "Response: ▁मैं<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: मुझे माफ कर दो\n",
            "Response: ▁मैं▁मैं<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: hello\n",
            "Response: ▁मैं▁मैं<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: how are you?\n",
            "Response: ▁मैं▁मैं<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "677f27fb",
        "outputId": "50d47091-7f27-44d4-d47c-a4bda4125c22"
      },
      "source": [
        "# Cell 8 - Train new SentencePiece model for combined Hinglish/English corpus\n",
        "from pathlib import Path\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "# Define paths - assuming corpus.txt now contains combined data from previous step\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "CORPUS = BASE / \"data\" / \"corpus.txt\" # Ensure this points to the combined corpus file\n",
        "\n",
        "# Ensure the models directory exists\n",
        "MODELS.mkdir(exist_ok=True)\n",
        "\n",
        "# Define output path and prefix for the new SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly_combined\") # New prefix for the larger model\n",
        "\n",
        "# Define a larger vocabulary size\n",
        "new_vocab_size = 2000 # Or 4000, or more, depending on dataset size\n",
        "\n",
        "# Train SentencePiece (larger vocab for combined data)\n",
        "print(f\"Training SentencePiece model with vocab size {new_vocab_size} on {CORPUS}...\")\n",
        "\n",
        "# Ensure SentencePiece model is trained only if it doesn't exist\n",
        "if not (Path(sp_prefix + \".model\")).exists():\n",
        "    try:\n",
        "        spm.SentencePieceTrainer.Train(\n",
        "            f\"--input={CORPUS} \"\n",
        "            f\"--model_prefix={sp_prefix} \"\n",
        "            f\"--vocab_size={new_vocab_size} \"\n",
        "            f\"--character_coverage=0.9995 \"\n",
        "            f\"--model_type=bpe \"\n",
        "            f\"--user_defined_symbols=<s>,</s>\"\n",
        "        )\n",
        "        print(\"Trained SPM:\", sp_prefix + \".model\")\n",
        "        print(\"SPM vocabulary size:\", spm.SentencePieceProcessor().Load(sp_prefix + \".model\").GetPieceSize())\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SentencePiece training: {e}\")\n",
        "else:\n",
        "    print(\"SentencePiece model already exists:\", sp_prefix + \".model\")\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SentencePiece model with vocab size 2000 on /content/bolly_chatbot/data/corpus.txt...\n",
            "Error during SentencePiece training: Internal: src/trainer_interface.cc(662) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (2000). Please set it to a value <= 129.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b91eff3",
        "outputId": "d72a5b4b-f2aa-4482-8be5-3947a3d435ee"
      },
      "source": [
        "# Cell 8 - Train new SentencePiece model for combined Hinglish/English corpus - Retry with smaller vocab size\n",
        "from pathlib import Path\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "# Define paths - assuming corpus.txt now contains combined data from previous step\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "CORPUS = BASE / \"data\" / \"corpus.txt\" # Ensure this points to the combined corpus file\n",
        "\n",
        "# Ensure the models directory exists\n",
        "MODELS.mkdir(exist_ok=True)\n",
        "\n",
        "# Define output path and prefix for the new SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly_combined\") # New prefix for the larger model\n",
        "\n",
        "# Define a smaller, acceptable vocabulary size based on the error message\n",
        "# The error suggested <= 129. Let's use 128 as it was used previously and worked.\n",
        "new_vocab_size = 128\n",
        "\n",
        "# Train SentencePiece (smaller vocab based on error)\n",
        "print(f\"Retrying SentencePiece model training with vocab size {new_vocab_size} on {CORPUS}...\")\n",
        "\n",
        "# Ensure SentencePiece model is trained only if it doesn't exist\n",
        "# Remove the existing model files if they are from a failed attempt with the same prefix\n",
        "model_file = Path(sp_prefix + \".model\")\n",
        "vocab_file = Path(sp_prefix + \".vocab\")\n",
        "if model_file.exists():\n",
        "    print(f\"Removing existing model file: {model_file}\")\n",
        "    model_file.unlink()\n",
        "if vocab_file.exists():\n",
        "    print(f\"Removing existing vocab file: {vocab_file}\")\n",
        "    vocab_file.unlink()\n",
        "\n",
        "\n",
        "try:\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        f\"--input={CORPUS} \"\n",
        "        f\"--model_prefix={sp_prefix} \"\n",
        "        f\"--vocab_size={new_vocab_size} \"\n",
        "        f\"--character_coverage=0.9995 \"\n",
        "        f\"--model_type=bpe \"\n",
        "        f\"--user_defined_symbols=<s>,</s>\"\n",
        "    )\n",
        "    print(\"Trained SPM:\", sp_prefix + \".model\")\n",
        "    # Load the trained model to get the actual vocabulary size\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.Load(sp_prefix + \".model\")\n",
        "    print(\"SPM vocabulary size:\", sp.GetPieceSize())\n",
        "except Exception as e:\n",
        "    print(f\"Error during SentencePiece training: {e}\")\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrying SentencePiece model training with vocab size 128 on /content/bolly_chatbot/data/corpus.txt...\n",
            "Trained SPM: /content/bolly_chatbot/models/spm_bolly_combined.model\n",
            "SPM vocabulary size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7e34bc6",
        "outputId": "5d99beea-d9d2-4ffd-b4b9-87630f474cdf"
      },
      "source": [
        "# Cell 9 - Train seq2seq model on new combined data\n",
        "\n",
        "# Define paths - assuming corpus.txt is updated and spm_bolly_combined.* exist\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "DATA = BASE / \"data\"\n",
        "MODELS = BASE / \"models\"\n",
        "CORPUS = DATA / \"corpus.txt\" # Should point to the combined corpus\n",
        "\n",
        "# Load the new SentencePiece model\n",
        "sp_prefix_combined = str(MODELS / \"spm_bolly_combined\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix_combined + \".model\")\n",
        "V = sp.GetPieceSize() # Get vocabulary size from the new model\n",
        "print(\"Loaded SPM model from:\", sp_prefix_combined + \".model\")\n",
        "print(\"SPM vocabulary size:\", V)\n",
        "\n",
        "\n",
        "# Model parameters (can keep same as tiny model for now, adjust if needed)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "print(f\"Model parameters: max_enc={max_enc}, max_dec={max_dec}, emb_dim={emb_dim}, units={units}, vocab_size={V}\")\n",
        "\n",
        "# Load the conversation pairs from the combined corpus\n",
        "lines = []\n",
        "# Re-read the corpus to get lines\n",
        "with open(CORPUS, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Rebuild pairs from lines - assuming corpus.txt is line-by-line\n",
        "pairs = []\n",
        "for i in range(0, len(lines), 2):\n",
        "    if i + 1 < len(lines):\n",
        "        pairs.append((lines[i], lines[i+1]))\n",
        "\n",
        "print(\"Pairs loaded from corpus:\", len(pairs))\n",
        "if len(pairs) == 0:\n",
        "    raise SystemExit(\"No pairs found in corpus.txt. Ensure the file is correctly formatted.\")\n",
        "\n",
        "\n",
        "# Encode the conversation pairs\n",
        "def encode(s, maxlen):\n",
        "    txt = \"<s> \" + s + \" </s>\"\n",
        "    encoded_ids = sp.EncodeAsIds(txt)\n",
        "    # Truncate if longer than maxlen\n",
        "    return encoded_ids[:maxlen]\n",
        "\n",
        "encs = []\n",
        "decins = []\n",
        "decouts = []\n",
        "\n",
        "for a, b in pairs:\n",
        "    e = encode(a, max_enc)\n",
        "    # Decoder input needs one less token than decoder output for shifted sequence\n",
        "    d_in = encode(b, max_dec)\n",
        "    d_out = encode(b, max_dec) # Max length for output sequence\n",
        "\n",
        "    # Ensure sequence is not empty after encoding/truncation\n",
        "    if len(e) == 0 or len(d_in) < 1 or len(d_out) < 1:\n",
        "         continue\n",
        "\n",
        "    # Shift decoder output by one token\n",
        "    # Decoder input sequence should not contain the </s> token\n",
        "    # Decoder output sequence should not contain the <s> token\n",
        "    # Pad d_in and d_out to max_dec\n",
        "    encs.append(e)\n",
        "    decins.append(d_in)\n",
        "    decouts.append(d_out)\n",
        "\n",
        "\n",
        "# Pad sequences\n",
        "encs = pad_sequences(encs, maxlen=max_enc, padding='post')\n",
        "decins = pad_sequences(decins, maxlen=max_dec, padding='post')\n",
        "decouts = pad_sequences(decouts, maxlen=max_dec, padding='post')\n",
        "\n",
        "# Reshape decouts for sparse_categorical_crossentropy\n",
        "decouts = np.expand_dims(decouts, -1)\n",
        "\n",
        "print(f\"Encoded samples: encs shape={encs.shape}, decins shape={decins.shape}, decouts shape={decouts.shape}\")\n",
        "\n",
        "\n",
        "# Define the seq2seq model architecture (ensure layer names match previous training)\n",
        "# Encoder\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "_, sh, sc = enc_lstm(emb_e)\n",
        "\n",
        "# Decoder\n",
        "dec_input = Input(shape=(max_dec,), name='dec_input')\n",
        "dec_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "# Add Reshape layer to explicitly set shape to (max_dec, emb_dim)\n",
        "reshaped_emb_d = Reshape((max_dec, emb_dim))(emb_d)\n",
        "dec_lstm = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs, _, _ = dec_lstm(reshaped_emb_d, initial_state=[sh, sc])\n",
        "dec_dense = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits = dec_dense(dec_outs)\n",
        "\n",
        "model_combined = Model([enc_input, dec_input], logits)\n",
        "\n",
        "# Compile the model\n",
        "model_combined.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "print(\"Training seq2seq model on combined data...\")\n",
        "# Train the model\n",
        "# Use a larger number of epochs and batch size if the dataset is significantly larger\n",
        "history = model_combined.fit([encs, decins], decouts, batch_size=32, epochs=10, verbose=1)\n",
        "\n",
        "# Save the trained model's weights\n",
        "weights_path_combined = MODELS / \"seq2seq_combined_weights.weights.h5\"\n",
        "model_combined.save_weights(str(weights_path_combined))\n",
        "print(\"Saved trained model weights to:\", weights_path_combined)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly_combined.model\n",
            "SPM vocabulary size: 128\n",
            "Model parameters: max_enc=16, max_dec=15, emb_dim=48, units=48, vocab_size=128\n",
            "Pairs loaded from corpus: 2\n",
            "Encoded samples: encs shape=(2, 16), decins shape=(2, 15), decouts shape=(2, 15, 1)\n",
            "Training seq2seq model on combined data...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_11' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 4.8536\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 4.8481\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 4.8424\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 4.8365\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 4.8303\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 4.8237\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 4.8166\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 4.8088\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 4.8001\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 4.7904\n",
            "Saved trained model weights to: /content/bolly_chatbot/models/seq2seq_combined_weights.weights.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb4d1034",
        "outputId": "36061ad2-e61e-4c3e-fc2c-92bf9f03f770"
      },
      "source": [
        "# Cell 10 - Inference with the new combined model\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define paths\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "# Load the newly trained SentencePiece model (step 1)\n",
        "sp_prefix_combined = str(MODELS / \"spm_bolly_combined\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix_combined + \".model\")\n",
        "V = sp.GetPieceSize() # Get vocabulary size from the new model\n",
        "print(\"Loaded SPM model from:\", sp_prefix_combined + \".model\")\n",
        "print(\"SPM vocabulary size:\", V)\n",
        "\n",
        "# Model parameters (should match combined model training)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "# Recreate the full model structure used during combined model training to load weights (step 2)\n",
        "# Ensure layer names match exactly\n",
        "# Encoder\n",
        "enc_input_full = Input(shape=(max_enc,), name='enc_input')\n",
        "# Use the updated vocabulary size V\n",
        "enc_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e_full = enc_emb_layer_full(enc_input_full)\n",
        "enc_lstm_full = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "enc_outputs_full, sh_full, sc_full = enc_lstm_full(emb_e_full)\n",
        "\n",
        "# Decoder\n",
        "dec_input_full = Input(shape=(max_dec,), name='dec_input')\n",
        "# Use the updated vocabulary size V\n",
        "dec_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d_full = dec_emb_layer_full(dec_input_full)\n",
        "# Add Reshape layer to explicitly set shape to (max_dec, emb_dim)\n",
        "reshaped_emb_d_full = Reshape((max_dec, emb_dim))(emb_d_full)\n",
        "dec_lstm_full = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs_full, _, _ = dec_lstm_full(reshaped_emb_d_full, initial_state=[sh_full, sc_full])\n",
        "dec_dense_full = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits_full = dec_dense_full(dec_outs_full)\n",
        "\n",
        "# Full model for loading weights\n",
        "full_model_combined = Model([enc_input_full, dec_input_full], logits_full)\n",
        "\n",
        "# Load trained weights into the full model (step 2)\n",
        "weights_path_combined = MODELS / \"seq2seq_combined_weights.weights.h5\"\n",
        "full_model_combined.load_weights(str(weights_path_combined))\n",
        "print(\"Loaded model weights into full model from:\", weights_path_combined)\n",
        "\n",
        "\n",
        "# Define the encoder model for inference (step 3)\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = full_model_combined.get_layer('enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = full_model_combined.get_layer('encoder_lstm')\n",
        "enc_outputs, state_h, state_c = enc_lstm(emb_e)\n",
        "encoder_model_combined = Model(enc_input, [state_h, state_c])\n",
        "print(\"Defined encoder model for inference\")\n",
        "\n",
        "# Define the decoder model for inference (step 3)\n",
        "dec_input = Input(shape=(1,), name='dec_input') # Decoder input is one token at a time\n",
        "dec_state_h = Input(shape=(units,), name='dec_state_h_input')\n",
        "dec_state_c = Input(shape=(units,), name='dec_state_c_input')\n",
        "dec_states_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "dec_emb_layer = full_model_combined.get_layer('dec_embedding') # Reuse embedding layer\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "\n",
        "dec_lstm = full_model_combined.get_layer('decoder_lstm') # Reuse LSTM layer\n",
        "dec_outputs, state_h_out, state_c_out = dec_lstm(emb_d, initial_state=dec_states_inputs)\n",
        "dec_states_outputs = [state_h_out, state_c_out]\n",
        "\n",
        "dec_dense = full_model_combined.get_layer('decoder_dense') # Reuse dense layer\n",
        "output_tokens = dec_dense(dec_outputs)\n",
        "\n",
        "decoder_model_combined = Model([dec_input] + dec_states_inputs, [output_tokens] + dec_states_outputs)\n",
        "print(\"Defined decoder model for inference\")\n",
        "\n",
        "# Define the inference function (step 4)\n",
        "def generate_response_combined(input_sentence, max_length=max_dec):\n",
        "    # Preprocess the input sentence using the combined SentencePiece model\n",
        "    input_seq = sp.EncodeAsIds(\"<s> \" + input_sentence + \" </s>\")\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_enc, padding='post')\n",
        "\n",
        "    # Get the initial states from the encoder (using the combined encoder model)\n",
        "    states_value = encoder_model_combined.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Start the decoder with the start token\n",
        "    start_token = sp.PieceToId(\"<s>\")\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        # Predict the next token (using the combined decoder model)\n",
        "        output_tokens, h, c = decoder_model_combined.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token (greedy approach for simplicity)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Add a check to ensure the sampled index is within the valid range\n",
        "        if sampled_token_index < 0 or sampled_token_index >= V:\n",
        "            print(f\"Warning: Sampled token index {sampled_token_index} is out of vocabulary range [0, {V-1}]. Stopping generation.\")\n",
        "            break\n",
        "\n",
        "        # Explicitly cast to int just in case and convert ID to piece\n",
        "        sampled_token = sp.IdToPiece(int(sampled_token_index))\n",
        "\n",
        "        # Exit condition: hitting stop character or max length\n",
        "        if sampled_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        # Append token\n",
        "        decoded_sentence.append(sampled_token)\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Join the tokens to form the sentence, handling SentencePiece subwords\n",
        "    return \"\".join(decoded_sentence).replace(\" \", \" \").strip()\n",
        "\n",
        "# Test the inference function with relevant inputs\n",
        "print(\"\\nTesting inference with combined model:\")\n",
        "sample_inputs = [\"क्या तुम मेरे साथ चलोगी?\", \"मुझे माफ कर दो\", \"hello\", \"how are you?\"] # Include English/Hinglish examples\n",
        "for sample_input in sample_inputs:\n",
        "    response = generate_response_combined(sample_input)\n",
        "    print(f\"Input: {sample_input}\\nResponse: {response}\\n\")\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly_combined.model\n",
            "SPM vocabulary size: 128\n",
            "Loaded model weights into full model from: /content/bolly_chatbot/models/seq2seq_combined_weights.weights.h5\n",
            "Defined encoder model for inference\n",
            "Defined decoder model for inference\n",
            "\n",
            "Testing inference with combined model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_13' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क्या तुम मेरे साथ चलोगी?\n",
            "Response: ▁▁▁<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: मुझे माफ कर दो\n",
            "Response: ▁मुझे▁मुझे▁मुझे▁मुझे▁मुझे<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: hello\n",
            "Response: ▁मुझे▁मुझे▁मुझे▁मुझे▁मुझे<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: how are you?\n",
            "Response: ▁▁▁<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8b75f8a",
        "outputId": "b9b92642-1d4e-400a-d93f-8fa8663ea9f2"
      },
      "source": [
        "# Cell 2 - Collect and prepare data - Update for new dataset\n",
        "\n",
        "# Assuming the new dataset is in the same /content/bolly_chatbot/data/ directory\n",
        "# and might include multiple .srt files or similar text files.\n",
        "# Modify the glob pattern if the new files have a different extension.\n",
        "from pathlib import Path\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "DATA = BASE / \"data\"\n",
        "MODELS = BASE / \"models\"\n",
        "CORPUS = DATA / \"corpus.txt\" # Changed to DATA / \"corpus.txt\"\n",
        "\n",
        "# import helpers\n",
        "import sys\n",
        "# Ensure the path is only appended if not already present\n",
        "if str(BASE) not in sys.path:\n",
        "    sys.path.append(str(BASE))\n",
        "from utils import parse_srt_to_lines, build_pairs_from_lines\n",
        "\n",
        "\n",
        "lines = []\n",
        "# Updated glob pattern to potentially include more files\n",
        "for p in DATA.glob(\"*.srt\"): # Adjust pattern if necessary, e.g., \"*.txt\" or \"*.*\"\n",
        "    print(f\"Parsing file: {p}\")\n",
        "    lines += parse_srt_to_lines(p)\n",
        "\n",
        "print(\"Lines found:\", len(lines))\n",
        "\n",
        "# Build pairs from the collected lines\n",
        "pairs = build_pairs_from_lines(lines)\n",
        "print(\"Pairs:\", len(pairs))\n",
        "\n",
        "if len(pairs) == 0:\n",
        "    raise SystemExit(\"No pairs found. Ensure the new dataset files are in /content/bolly_chatbot/data/ and have the correct file extension.\")\n",
        "\n",
        "# Write the updated corpus\n",
        "with open(CORPUS, \"w\", encoding=\"utf-8\") as f:\n",
        "    for a, b in pairs:\n",
        "        f.write(a + \"\\n\")\n",
        "        f.write(b + \"\\n\")\n",
        "print(\"Wrote updated corpus:\", CORPUS)\n",
        "\n",
        "# Display the first few pairs to verify\n",
        "print(\"\\nSample pairs from the new dataset:\")\n",
        "for i, pair in enumerate(pairs[:5]):\n",
        "    print(f\"Pair {i+1}: {pair}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing file: /content/bolly_chatbot/data/sample_subs.srt\n",
            "Lines found: 3\n",
            "Pairs: 2\n",
            "Wrote updated corpus: /content/bolly_chatbot/data/corpus.txt\n",
            "\n",
            "Sample pairs from the new dataset:\n",
            "Pair 1: ('क्या तुम मेरे साथ चलोगी?', 'क्यों नहीं, मैं आ रही हूँ।')\n",
            "Pair 2: ('क्यों नहीं, मैं आ रही हूँ।', 'मुझे माफ कर दो, मेरी गलती थी।')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "935c78e9",
        "outputId": "523c4723-c2e2-4703-f7db-100b27cfef3a"
      },
      "source": [
        "# Cell 3 - Train small SentencePiece and tiny seq2seq (fast) - Retry with updated data\n",
        "\n",
        "# Ensure SentencePiece model is trained only if it doesn't exist or force retraining\n",
        "# For retraining with new data, we should overwrite the existing model\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "spm.SentencePieceTrainer.Train(f\"--input={CORPUS} --model_prefix={sp_prefix} --vocab_size=128 --character_coverage=0.9995 --model_type=bpe --user_defined_symbols=<s>,</s>\")\n",
        "print(\"Trained SPM:\", sp_prefix + \".model\")\n",
        "\n",
        "\n",
        "# prepare encoded data\n",
        "sp = spm.SentencePieceProcessor(); sp.Load(sp_prefix + \".model\")\n",
        "V = sp.GetPieceSize() # Update vocabulary size based on the newly trained model\n",
        "print(\"SPM vocab size:\", V)\n",
        "\n",
        "def encode(s, maxlen=16):\n",
        "    txt = \"<s> \" + s + \" </s>\"\n",
        "    encoded_ids = sp.EncodeAsIds(txt)\n",
        "    # Ensure truncation happens if needed\n",
        "    return encoded_ids[:maxlen]\n",
        "\n",
        "max_enc=16; max_dec=15\n",
        "encs=[]; decins=[]; decouts=[]\n",
        "for a,b in pairs:\n",
        "    e = encode(a,max_enc)\n",
        "    d = encode(b,max_dec+1)\n",
        "    if len(d) < 2: continue # Skip empty or single-token decoder sequences\n",
        "    encs.append(e)\n",
        "    decins.append(d[:-1])\n",
        "    decouts.append(d[1:])\n",
        "\n",
        "# Convert to numpy arrays and pad\n",
        "# Ensure there are sequences to process before padding\n",
        "if not encs:\n",
        "    raise SystemExit(\"No valid sequences generated from pairs. Check data and encoding.\")\n",
        "\n",
        "encs = pad_sequences(encs, maxlen=max_enc, padding='post')\n",
        "decins = pad_sequences(decins, maxlen=max_dec, padding='post')\n",
        "decouts = pad_sequences(decouts, maxlen=max_dec, padding='post')\n",
        "decouts = np.expand_dims(decouts, -1) # Add a dimension for sparse_categorical_crossentropy\n",
        "\n",
        "print(f\"Prepared {len(encs)} encoded sequences.\")\n",
        "print(\"Encoder shape:\", encs.shape)\n",
        "print(\"Decoder input shape:\", decins.shape)\n",
        "print(\"Decoder output shape:\", decouts.shape)\n",
        "\n",
        "\n",
        "# define tiny seq2seq model\n",
        "# Reuse model definition from previous attempts, ensuring layer names match\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "emb_dim=48; units=48 # Use the same dimensions as before\n",
        "\n",
        "# Encoder\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "# Use the updated vocabulary size V\n",
        "enc_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "_, sh, sc = enc_lstm(emb_e) # Don't need the full sequence output from encoder LSTM\n",
        "\n",
        "# Decoder\n",
        "dec_input = Input(shape=(max_dec,), name='dec_input')\n",
        "# Use the updated vocabulary size V\n",
        "dec_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "# Add Reshape layer to explicitly set shape if needed (matching training structure)\n",
        "# Check if this Reshape was truly necessary or caused warnings/errors before.\n",
        "# Based on previous runs, the warning about mask loss suggests it might be\n",
        "# better to remove if possible, but let's keep it for now to match the structure\n",
        "# that successfully loaded weights previously.\n",
        "# If the model doesn't train or predict correctly, this might be a place to revisit.\n",
        "# Let's add the Reshape back as it was in the successful inference attempts.\n",
        "reshaped_emb_d = Reshape((max_dec, emb_dim))(emb_d)\n",
        "\n",
        "dec_lstm = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "# Initial state should be from the encoder\n",
        "dec_outs, _, _ = dec_lstm(reshaped_emb_d, initial_state=[sh, sc])\n",
        "# Use the updated vocabulary size V\n",
        "dec_dense = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits = dec_dense(dec_outs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([enc_input, dec_input], logits)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Print model summary to verify structure\n",
        "model.summary()\n",
        "\n",
        "\n",
        "print(\"Training tiny model...\")\n",
        "# Train the model on the new data\n",
        "# Adjust epochs and batch size if needed based on dataset size and performance\n",
        "model.fit([encs, decins], decouts, batch_size=4, epochs=10, verbose=2) # Increased epochs slightly\n",
        "\n",
        "\n",
        "# save artifacts\n",
        "# Save weights after successful training\n",
        "model.save_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "# SentencePiece model is already saved during the SentencePieceTrainer.Train call\n",
        "print(\"Saved weights to\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "print(\"SPM vocab size:\", sp.GetPieceSize())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained SPM: /content/bolly_chatbot/models/spm_bolly.model\n",
            "SPM vocab size: 128\n",
            "Prepared 2 encoded sequences.\n",
            "Encoder shape: (2, 16)\n",
            "Decoder input shape: (2, 15)\n",
            "Decoder output shape: (2, 15, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_14' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_37\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_37\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ dec_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m48\u001b[0m)    │      \u001b[38;5;34m6,144\u001b[0m │ dec_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m48\u001b[0m)    │      \u001b[38;5;34m6,144\u001b[0m │ enc_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_56        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ enc_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m48\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dec_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m),      │     \u001b[38;5;34m18,624\u001b[0m │ enc_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m),       │            │ not_equal_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m48\u001b[0m),  │     \u001b[38;5;34m18,624\u001b[0m │ reshape_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m),       │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)]       │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m6,272\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ dec_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │ dec_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │ enc_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_56        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enc_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,624</span> │ enc_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),       │            │ not_equal_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,624</span> │ reshape_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>),       │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)]       │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,808\u001b[0m (218.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,808</span> (218.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,808\u001b[0m (218.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,808</span> (218.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tiny model...\n",
            "Epoch 1/10\n",
            "1/1 - 4s - 4s/step - loss: 4.8507\n",
            "Epoch 2/10\n",
            "1/1 - 0s - 57ms/step - loss: 4.8441\n",
            "Epoch 3/10\n",
            "1/1 - 0s - 56ms/step - loss: 4.8374\n",
            "Epoch 4/10\n",
            "1/1 - 0s - 63ms/step - loss: 4.8304\n",
            "Epoch 5/10\n",
            "1/1 - 0s - 72ms/step - loss: 4.8230\n",
            "Epoch 6/10\n",
            "1/1 - 0s - 59ms/step - loss: 4.8150\n",
            "Epoch 7/10\n",
            "1/1 - 0s - 60ms/step - loss: 4.8062\n",
            "Epoch 8/10\n",
            "1/1 - 0s - 56ms/step - loss: 4.7966\n",
            "Epoch 9/10\n",
            "1/1 - 0s - 62ms/step - loss: 4.7858\n",
            "Epoch 10/10\n",
            "1/1 - 0s - 59ms/step - loss: 4.7737\n",
            "Saved weights to /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "SPM vocab size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90604f77",
        "outputId": "8693982e-b658-4179-dd91-c675ebc58fec"
      },
      "source": [
        "# Cell 4 - Inference - Test with retrained model\n",
        "# Reusing the inference code from previous attempts\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix + \".model\")\n",
        "V = sp.GetPieceSize()\n",
        "print(\"Loaded SPM model from:\", sp_prefix + \".model\")\n",
        "print(\"SPM vocab size:\", V)\n",
        "\n",
        "# Model parameters (should match training)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "# Recreate the full model structure used during training to load weights\n",
        "# Ensure layer names match exactly\n",
        "# Encoder\n",
        "enc_input_full = Input(shape=(max_enc,), name='enc_input')\n",
        "# Use the updated vocabulary size V\n",
        "enc_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e_full = enc_emb_layer_full(enc_input_full)\n",
        "enc_lstm_full = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "enc_outputs_full, sh_full, sc_full = enc_lstm_full(emb_e_full)\n",
        "\n",
        "# Decoder\n",
        "dec_input_full = Input(shape=(max_dec,), name='dec_input')\n",
        "# Use the updated vocabulary size V\n",
        "dec_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d_full = dec_emb_layer_full(dec_input_full)\n",
        "# Add Reshape layer as used in training\n",
        "reshaped_emb_d_full = Reshape((max_dec, emb_dim))(emb_d_full)\n",
        "dec_lstm_full = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs_full, _, _ = dec_lstm_full(reshaped_emb_d_full, initial_state=[sh_full, sc_full])\n",
        "dec_dense_full = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits_full = dec_dense_full(dec_outs_full)\n",
        "\n",
        "# Full model for loading weights\n",
        "full_model = Model([enc_input_full, dec_input_full], logits_full)\n",
        "\n",
        "# Load trained weights into the full model\n",
        "full_model.load_weights(str(MODELS / \"seq2seq_weights.weights.h5\"))\n",
        "print(\"Loaded model weights into full model from:\", MODELS / \"seq2seq_weights.weights.h5\")\n",
        "\n",
        "\n",
        "# Now define the encoder model for inference\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = full_model.get_layer('enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = full_model.get_layer('encoder_lstm')\n",
        "enc_outputs, state_h, state_c = enc_lstm(emb_e)\n",
        "encoder_model = Model(enc_input, [state_h, state_c])\n",
        "print(\"Defined encoder model for inference\")\n",
        "\n",
        "# Define the decoder model for inference\n",
        "dec_input = Input(shape=(1,), name='dec_input') # Decoder input is one token at a time\n",
        "dec_state_h = Input(shape=(units,), name='dec_state_h_input')\n",
        "dec_state_c = Input(shape=(units,), name='dec_state_c_input')\n",
        "dec_states_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "dec_emb_layer = full_model.get_layer('dec_embedding') # Reuse embedding layer\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "\n",
        "dec_lstm = full_model.get_layer('decoder_lstm') # Reuse LSTM layer\n",
        "dec_outputs, state_h_out, state_c_out = dec_lstm(emb_d, initial_state=dec_states_inputs)\n",
        "dec_states_outputs = [state_h_out, state_c_out]\n",
        "\n",
        "dec_dense = full_model.get_layer('decoder_dense') # Reuse dense layer\n",
        "output_tokens = dec_dense(dec_outputs)\n",
        "\n",
        "decoder_model = Model([dec_input] + dec_states_inputs, [output_tokens] + dec_states_outputs)\n",
        "print(\"Defined decoder model for inference\")\n",
        "\n",
        "\n",
        "# Define the inference function\n",
        "def generate_response(input_sentence, max_length=max_dec):\n",
        "    # Preprocess the input sentence\n",
        "    input_seq = sp.EncodeAsIds(\"<s> \" + input_sentence + \" </s>\")\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_enc, padding='post')\n",
        "\n",
        "    # Get the initial states from the encoder\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Start the decoder with the start token\n",
        "    start_token = sp.PieceToId(\"<s>\")\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token (greedy approach for simplicity)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Add a check to ensure the sampled index is within the valid range\n",
        "        if sampled_token_index < 0 or sampled_token_index >= V:\n",
        "            print(f\"Warning: Sampled token index {sampled_token_index} is out of vocabulary range [0, {V-1}]. Stopping generation.\")\n",
        "            break\n",
        "\n",
        "        # Explicitly cast to int just in case\n",
        "        sampled_token = sp.IdToPiece(int(sampled_token_index))\n",
        "\n",
        "        # Exit condition: hitting stop character or max length\n",
        "        if sampled_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        # Append token\n",
        "        decoded_sentence.append(sampled_token)\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Join the tokens to form the sentence, handling SentencePiece subwords\n",
        "    return \"\".join(decoded_sentence).replace(\" \", \" \").strip()\n",
        "\n",
        "# Test the inference function\n",
        "print(\"\\nTesting inference:\")\n",
        "# Use sample inputs relevant to the (potentially new) dataset\n",
        "sample_inputs = [\"क्या तुम मेरे साथ चलोगी?\", \"मुझे माफ कर दो\", \"hello\", \"how are you?\"]\n",
        "for sample_input in sample_inputs:\n",
        "    response = generate_response(sample_input)\n",
        "    print(f\"Input: {sample_input}\\nResponse: {response}\\n\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly.model\n",
            "SPM vocab size: 128\n",
            "Loaded model weights into full model from: /content/bolly_chatbot/models/seq2seq_weights.weights.h5\n",
            "Defined encoder model for inference\n",
            "Defined decoder model for inference\n",
            "\n",
            "Testing inference:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_15' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क्या तुम मेरे साथ चलोगी?\n",
            "Response: <s><s><s><s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: मुझे माफ कर दो\n",
            "Response: <s><s><s><s>▁मैं▁हूँ<unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: hello\n",
            "Response: <s><s><s><s>▁मैं▁हूँ<unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: how are you?\n",
            "Response: <s><s><s><s><s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d49592bd",
        "outputId": "114c1c2c-ce34-42c3-c07b-ee2371cc34e5"
      },
      "source": [
        "# Cell 8 - Train new SentencePiece model for combined Hinglish/English corpus\n",
        "from pathlib import Path\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "# Define paths - assuming corpus.txt now contains combined data from previous step\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "CORPUS = BASE / \"data\" / \"corpus.txt\" # Ensure this points to the combined corpus file\n",
        "\n",
        "# Ensure the models directory exists\n",
        "MODELS.mkdir(exist_ok=True)\n",
        "\n",
        "# Define output path and prefix for the new SentencePiece model\n",
        "sp_prefix = str(MODELS / \"spm_bolly_combined\") # New prefix for the larger model\n",
        "\n",
        "# Define a larger vocabulary size\n",
        "new_vocab_size = 2000 # Or 4000, or more, depending on dataset size\n",
        "\n",
        "# Train SentencePiece (larger vocab for combined data)\n",
        "print(f\"Training SentencePiece model with vocab size {new_vocab_size} on {CORPUS}...\")\n",
        "\n",
        "# Ensure SentencePiece model is trained only if it doesn't exist\n",
        "if not (Path(sp_prefix + \".model\")).exists():\n",
        "    try:\n",
        "        spm.SentencePieceTrainer.Train(\n",
        "            f\"--input={CORPUS} \"\n",
        "            f\"--model_prefix={sp_prefix} \"\n",
        "            f\"--vocab_size={new_vocab_size} \"\n",
        "            f\"--character_coverage=0.9995 \"\n",
        "            f\"--model_type=bpe \"\n",
        "            f\"--user_defined_symbols=<s>,</s>\"\n",
        "        )\n",
        "        print(\"Trained SPM:\", sp_prefix + \".model\")\n",
        "        print(\"SPM vocabulary size:\", spm.SentencePieceProcessor().Load(sp_prefix + \".model\").GetPieceSize())\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SentencePiece training: {e}\")\n",
        "else:\n",
        "    print(\"SentencePiece model already exists:\", sp_prefix + \".model\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SentencePiece model with vocab size 2000 on /content/bolly_chatbot/data/corpus.txt...\n",
            "SentencePiece model already exists: /content/bolly_chatbot/models/spm_bolly_combined.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbc7b2d7",
        "outputId": "ce96158b-af46-4ced-da02-2f2435da5472"
      },
      "source": [
        "# Cell 9 - Train seq2seq model on new combined data\n",
        "\n",
        "# Define paths - assuming corpus.txt is updated and spm_bolly_combined.* exist\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "DATA = BASE / \"data\"\n",
        "MODELS = BASE / \"models\"\n",
        "CORPUS = DATA / \"corpus.txt\" # Should point to the combined corpus\n",
        "\n",
        "# Load the new SentencePiece model\n",
        "sp_prefix_combined = str(MODELS / \"spm_bolly_combined\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix_combined + \".model\")\n",
        "V = sp.GetPieceSize() # Get vocabulary size from the new model\n",
        "print(\"Loaded SPM model from:\", sp_prefix_combined + \".model\")\n",
        "print(\"SPM vocabulary size:\", V)\n",
        "\n",
        "\n",
        "# Model parameters (can keep same as tiny model for now, adjust if needed)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "print(f\"Model parameters: max_enc={max_enc}, max_dec={max_dec}, emb_dim={emb_dim}, units={units}, vocab_size={V}\")\n",
        "\n",
        "# Load the conversation pairs from the combined corpus\n",
        "lines = []\n",
        "# Re-read the corpus to get lines\n",
        "with open(CORPUS, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Rebuild pairs from lines - assuming corpus.txt is line-by-line\n",
        "pairs = []\n",
        "for i in range(0, len(lines), 2):\n",
        "    if i + 1 < len(lines):\n",
        "        pairs.append((lines[i], lines[i+1]))\n",
        "\n",
        "print(\"Pairs loaded from corpus:\", len(pairs))\n",
        "if len(pairs) == 0:\n",
        "    raise SystemExit(\"No pairs found in corpus.txt. Ensure the file is correctly formatted.\")\n",
        "\n",
        "\n",
        "# Encode the conversation pairs\n",
        "def encode(s, maxlen):\n",
        "    txt = \"<s> \" + s + \" </s>\"\n",
        "    encoded_ids = sp.EncodeAsIds(txt)\n",
        "    # Truncate if longer than maxlen\n",
        "    return encoded_ids[:maxlen]\n",
        "\n",
        "encs = []\n",
        "decins = []\n",
        "decouts = []\n",
        "\n",
        "for a, b in pairs:\n",
        "    e = encode(a, max_enc)\n",
        "    # Decoder input needs one less token than decoder output for shifted sequence\n",
        "    d_in = encode(b, max_dec)\n",
        "    d_out = encode(b, max_dec) # Max length for output sequence\n",
        "\n",
        "    # Ensure sequence is not empty after encoding/truncation\n",
        "    if len(e) == 0 or len(d_in) < 1 or len(d_out) < 1:\n",
        "         continue\n",
        "\n",
        "    # Shift decoder output by one token\n",
        "    # Decoder input sequence should not contain the </s> token\n",
        "    # Decoder output sequence should not contain the <s> token\n",
        "    # Pad d_in and d_out to max_dec\n",
        "    encs.append(e)\n",
        "    decins.append(d_in)\n",
        "    decouts.append(d_out)\n",
        "\n",
        "\n",
        "# Pad sequences\n",
        "encs = pad_sequences(encs, maxlen=max_enc, padding='post')\n",
        "decins = pad_sequences(decins, maxlen=max_dec, padding='post')\n",
        "decouts = pad_sequences(decouts, maxlen=max_dec, padding='post')\n",
        "\n",
        "# Reshape decouts for sparse_categorical_crossentropy\n",
        "decouts = np.expand_dims(decouts, -1)\n",
        "\n",
        "print(f\"Encoded samples: encs shape={encs.shape}, decins shape={decins.shape}, decouts shape={decouts.shape}\")\n",
        "\n",
        "\n",
        "# Define the seq2seq model architecture (ensure layer names match previous training)\n",
        "# Encoder\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "_, sh, sc = enc_lstm(emb_e)\n",
        "\n",
        "# Decoder\n",
        "dec_input = Input(shape=(max_dec,), name='dec_input')\n",
        "dec_emb_layer = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "# Add Reshape layer to explicitly set shape to (max_dec, emb_dim)\n",
        "reshaped_emb_d = Reshape((max_dec, emb_dim))(emb_d)\n",
        "dec_lstm = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs, _, _ = dec_lstm(reshaped_emb_d, initial_state=[sh, sc])\n",
        "dec_dense = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits = dec_dense(dec_outs)\n",
        "\n",
        "model_combined = Model([enc_input, dec_input], logits)\n",
        "\n",
        "# Compile the model\n",
        "model_combined.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "print(\"Training seq2seq model on combined data...\")\n",
        "# Train the model\n",
        "# Use a larger number of epochs and batch size if the dataset is significantly larger\n",
        "history = model_combined.fit([encs, decins], decouts, batch_size=32, epochs=10, verbose=1)\n",
        "\n",
        "# Save the trained model's weights\n",
        "weights_path_combined = MODELS / \"seq2seq_combined_weights.weights.h5\"\n",
        "model_combined.save_weights(str(weights_path_combined))\n",
        "print(\"Saved trained model weights to:\", weights_path_combined)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly_combined.model\n",
            "SPM vocabulary size: 128\n",
            "Model parameters: max_enc=16, max_dec=15, emb_dim=48, units=48, vocab_size=128\n",
            "Pairs loaded from corpus: 2\n",
            "Encoded samples: encs shape=(2, 16), decins shape=(2, 15), decouts shape=(2, 15, 1)\n",
            "Training seq2seq model on combined data...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_16' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 4.8510\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 4.8446\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 4.8380\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 4.8311\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 4.8236\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 4.8155\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 4.8066\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 4.7966\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 4.7852\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 4.7723\n",
            "Saved trained model weights to: /content/bolly_chatbot/models/seq2seq_combined_weights.weights.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d14dc94",
        "outputId": "3e770dca-a919-4818-d6d3-a1b4a9c6feca"
      },
      "source": [
        "# Cell 10 - Inference with the new combined model\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define paths\n",
        "BASE = Path(\"/content/bolly_chatbot\")\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "# Load the newly trained SentencePiece model (step 1)\n",
        "sp_prefix_combined = str(MODELS / \"spm_bolly_combined\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(sp_prefix_combined + \".model\")\n",
        "V = sp.GetPieceSize() # Get vocabulary size from the new model\n",
        "print(\"Loaded SPM model from:\", sp_prefix_combined + \".model\")\n",
        "print(\"SPM vocabulary size:\", V)\n",
        "\n",
        "# Model parameters (should match combined model training)\n",
        "max_enc = 16\n",
        "max_dec = 15\n",
        "emb_dim = 48\n",
        "units = 48\n",
        "\n",
        "# Recreate the full model structure used during combined model training to load weights (step 2)\n",
        "# Ensure layer names match exactly\n",
        "# Encoder\n",
        "enc_input_full = Input(shape=(max_enc,), name='enc_input')\n",
        "# Use the updated vocabulary size V\n",
        "enc_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='enc_embedding')\n",
        "emb_e_full = enc_emb_layer_full(enc_input_full)\n",
        "enc_lstm_full = LSTM(units, return_state=True, name='encoder_lstm')\n",
        "enc_outputs_full, sh_full, sc_full = enc_lstm_full(emb_e_full)\n",
        "\n",
        "# Decoder\n",
        "dec_input_full = Input(shape=(max_dec,), name='dec_input')\n",
        "# Use the updated vocabulary size V\n",
        "dec_emb_layer_full = Embedding(V, emb_dim, mask_zero=True, name='dec_embedding')\n",
        "emb_d_full = dec_emb_layer_full(dec_input_full)\n",
        "# Add Reshape layer to explicitly set shape to (max_dec, emb_dim)\n",
        "reshaped_emb_d_full = Reshape((max_dec, emb_dim))(emb_d_full)\n",
        "dec_lstm_full = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "dec_outs_full, _, _ = dec_lstm_full(reshaped_emb_d_full, initial_state=[sh_full, sc_full])\n",
        "dec_dense_full = Dense(V, activation='softmax', name='decoder_dense')\n",
        "logits_full = dec_dense_full(dec_outs_full)\n",
        "\n",
        "# Full model for loading weights\n",
        "full_model_combined = Model([enc_input_full, dec_input_full], logits_full)\n",
        "\n",
        "# Load trained weights into the full model (step 2)\n",
        "weights_path_combined = MODELS / \"seq2seq_combined_weights.weights.h5\"\n",
        "full_model_combined.load_weights(str(weights_path_combined))\n",
        "print(\"Loaded model weights into full model from:\", weights_path_combined)\n",
        "\n",
        "\n",
        "# Define the encoder model for inference (step 3)\n",
        "enc_input = Input(shape=(max_enc,), name='enc_input')\n",
        "enc_emb_layer = full_model_combined.get_layer('enc_embedding')\n",
        "emb_e = enc_emb_layer(enc_input)\n",
        "enc_lstm = full_model_combined.get_layer('encoder_lstm')\n",
        "enc_outputs, state_h, state_c = enc_lstm(emb_e)\n",
        "encoder_model_combined = Model(enc_input, [state_h, state_c])\n",
        "print(\"Defined encoder model for inference\")\n",
        "\n",
        "# Define the decoder model for inference (step 3)\n",
        "dec_input = Input(shape=(1,), name='dec_input') # Decoder input is one token at a time\n",
        "dec_state_h = Input(shape=(units,), name='dec_state_h_input')\n",
        "dec_state_c = Input(shape=(units,), name='dec_state_c_input')\n",
        "dec_states_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "dec_emb_layer = full_model_combined.get_layer('dec_embedding') # Reuse embedding layer\n",
        "emb_d = dec_emb_layer(dec_input)\n",
        "\n",
        "dec_lstm = full_model_combined.get_layer('decoder_lstm') # Reuse LSTM layer\n",
        "dec_outputs, state_h_out, state_c_out = dec_lstm(emb_d, initial_state=dec_states_inputs)\n",
        "dec_states_outputs = [state_h_out, state_c_out]\n",
        "\n",
        "dec_dense = full_model_combined.get_layer('decoder_dense') # Reuse dense layer\n",
        "output_tokens = dec_dense(dec_outputs)\n",
        "\n",
        "decoder_model_combined = Model([dec_input] + dec_states_inputs, [output_tokens] + dec_states_outputs)\n",
        "print(\"Defined decoder model for inference\")\n",
        "\n",
        "# Define the inference function (step 4)\n",
        "def generate_response_combined(input_sentence, max_length=max_dec):\n",
        "    # Preprocess the input sentence using the combined SentencePiece model\n",
        "    input_seq = sp.EncodeAsIds(\"<s> \" + input_sentence + \" </s>\")\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_enc, padding='post')\n",
        "\n",
        "    # Get the initial states from the encoder (using the combined encoder model)\n",
        "    states_value = encoder_model_combined.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Start the decoder with the start token\n",
        "    start_token = sp.PieceToId(\"<s>\")\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        # Predict the next token (using the combined decoder model)\n",
        "        output_tokens, h, c = decoder_model_combined.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token (greedy approach for simplicity)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Add a check to ensure the sampled index is within the valid range\n",
        "        if sampled_token_index < 0 or sampled_token_index >= V:\n",
        "            print(f\"Warning: Sampled token index {sampled_token_index} is out of vocabulary range [0, {V-1}]. Stopping generation.\")\n",
        "            break\n",
        "\n",
        "        # Explicitly cast to int just in case and convert ID to piece\n",
        "        sampled_token = sp.IdToPiece(int(sampled_token_index))\n",
        "\n",
        "        # Exit condition: hitting stop character or max length\n",
        "        if sampled_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        # Append token\n",
        "        decoded_sentence.append(sampled_token)\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Join the tokens to form the sentence, handling SentencePiece subwords\n",
        "    return \"\".join(decoded_sentence).replace(\" \", \" \").strip()\n",
        "\n",
        "# Test the inference function with relevant inputs\n",
        "print(\"\\nTesting inference with combined model:\")\n",
        "sample_inputs = [\"क्या तुम मेरे साथ चलोगी?\", \"मुझे माफ कर दो\", \"hello\", \"how are you?\"] # Include English/Hinglish examples\n",
        "for sample_input in sample_inputs:\n",
        "    response = generate_response_combined(sample_input)\n",
        "    print(f\"Input: {sample_input}\\nResponse: {response}\\n\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SPM model from: /content/bolly_chatbot/models/spm_bolly_combined.model\n",
            "SPM vocabulary size: 128\n",
            "Loaded model weights into full model from: /content/bolly_chatbot/models/seq2seq_combined_weights.weights.h5\n",
            "Defined encoder model for inference\n",
            "Defined decoder model for inference\n",
            "\n",
            "Testing inference with combined model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'reshape_17' (of type Reshape) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क्या तुम मेरे साथ चलोगी?\n",
            "Response: ▁▁▁▁▁▁▁▁<unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: मुझे माफ कर दो\n",
            "Response: ▁▁▁▁▁▁▁▁<unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: hello\n",
            "Response: ▁▁▁▁▁▁▁▁<unk><unk><unk><unk><unk><unk><unk>\n",
            "\n",
            "Input: how are you?\n",
            "Response: ▁▁▁▁▁▁▁▁<unk><unk><unk><unk><unk><unk><unk>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}